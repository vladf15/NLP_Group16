{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cd4754415aff4591b1eab9aee3d1f8ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_348144356e154e01a0f789407ab67d42",
              "IPY_MODEL_b09735fc4bff4802b20f4d662a488f7a",
              "IPY_MODEL_c20cb59c6fdf45878635099c5652ba5c"
            ],
            "layout": "IPY_MODEL_f901b5dcd5f940e3b3e6e777c74790a6"
          }
        },
        "348144356e154e01a0f789407ab67d42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_004f200aa5a548d4b3073cb6358b4e1d",
            "placeholder": "​",
            "style": "IPY_MODEL_33569b95067f41fdadf13e09333a4937",
            "value": "Downloading builder script: "
          }
        },
        "b09735fc4bff4802b20f4d662a488f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_812e1da4636e44b49882b43e3c1a0f8b",
            "max": 2318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9ff20d748fc40938e39136a87f88d60",
            "value": 2318
          }
        },
        "c20cb59c6fdf45878635099c5652ba5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aef22deb20b941349b47bcc4da7fdd5b",
            "placeholder": "​",
            "style": "IPY_MODEL_d9d5ae0688fc46808c04dca7550cdfef",
            "value": " 6.50k/? [00:00&lt;00:00, 533kB/s]"
          }
        },
        "f901b5dcd5f940e3b3e6e777c74790a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "004f200aa5a548d4b3073cb6358b4e1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33569b95067f41fdadf13e09333a4937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "812e1da4636e44b49882b43e3c1a0f8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9ff20d748fc40938e39136a87f88d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aef22deb20b941349b47bcc4da7fdd5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9d5ae0688fc46808c04dca7550cdfef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Configuration"
      ],
      "metadata": {
        "id": "vw-1P3hD67lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"google/flan-t5-large\"\n",
        "\n",
        "BASE_LOCATION = \"/content/drive/My Drive/\"\n",
        "DATA_LOCATION = BASE_LOCATION + \"dataset-decomposed.jsonl\"\n",
        "\n",
        "LEARNING_RATE = 1e-5\n",
        "BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "AWugDuNU6-fN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install dependencies"
      ],
      "metadata": {
        "id": "q5fWs6Zv6bwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade transformers datasets\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbsVwyaJCLyO",
        "outputId": "cd7c5bfc-45ea-4a90-86c7-42a7449107c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting accelerate\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load model"
      ],
      "metadata": {
        "id": "tQ1kHcvj6lb4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9sGXbfPBnAU"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Load dataset"
      ],
      "metadata": {
        "id": "k1ImiiL76xyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EuG5JnkJ7bi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "from ast import literal_eval\n",
        "\n",
        "# train_dataset = pd.read_csv(TRAIN_LOCATION, index_col=0)\n",
        "# train_dataset['evidences'] = train_dataset['evidences'].apply(literal_eval)\n",
        "# train_dataset['scores'] = train_dataset['scores'].apply(literal_eval)\n",
        "# train_dataset = Dataset.from_pandas(train_dataset)\n",
        "\n",
        "dataset = load_dataset('json', data_files=[DATA_LOCATION])\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['test']\n",
        "\n",
        "# train()\n",
        "\n",
        "# val_dataset = pd.read_csv(VAL_LOCATION, index_col=0)\n",
        "# val_dataset['evidences'] = val_dataset['evidences'].apply(literal_eval)\n",
        "# val_dataset['scores'] = val_dataset['scores'].apply(literal_eval)\n",
        "# val_dataset = Dataset.from_pandas(val_dataset)"
      ],
      "metadata": {
        "id": "qqBl9qSBEi6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw1QzTSSFHDH",
        "outputId": "073d1595-d4ad-4195-fc82-18b8a711f4ca"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['label', 'claim', 'subquestions', 'subquestions_retrieve', 'claim_evidences', 'subquestions_evidence', 'combined_evidence'],\n",
              "    num_rows: 100\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Preprocess data"
      ],
      "metadata": {
        "id": "2direlK28J8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(claim, evidences, subquestions):\n",
        "    prompt_begin = (\n",
        "        f\"You are a fact-checker. Your task is to classify the claim based on the following evidence.\\n\\n\"\n",
        "        f\"Claim: {claim}\\n\\n\"\n",
        "        f\"{subquestions}\\n\\n\"\n",
        "        f\"Evidences:\\n\"\n",
        "    )\n",
        "    prompt_evidences = f\"\"\n",
        "    prompt_end = f\"\\n\\nLabel (TRUE, FALSE, CONFLICTING):\"\n",
        "\n",
        "    for k in range(0, min(5, len(evidences))):\n",
        "        new_evidence = f\"\\n{k + 1}. {evidences[k]}\"\n",
        "        tokens = tokenizer(prompt_begin + prompt_evidences + new_evidence + prompt_end, max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "        if tokens['input_ids'].shape[1] < tokenizer.model_max_length:\n",
        "            prompt_evidences += new_evidence\n",
        "        else:\n",
        "            break\n",
        "    return prompt_begin + prompt_evidences + prompt_end\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [generate_prompt(examples['claim'][i], examples['combined_evidence'][i], '\\n'.join(examples['subquestions'][i])) for i in range(len(examples[\"claim\"]))]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(text_target=examples[\"label\"], max_length=16, padding='max_length')\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "aJdgFm3J7Qaw"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_validation = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)"
      ],
      "metadata": {
        "id": "WuCX6Ia5cVHZ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset['combined_evidence'][0][0]\n",
        "\n",
        "print(generate_prompt(train_dataset['claim'][0], train_dataset['combined_evidence'][0], '\\n'.join(train_dataset['subquestions'][0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjjeQFtCHlKm",
        "outputId": "1fe25dbb-e820-4d82-dbe8-4bbb61b46a95"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a fact-checker. Your task is to classify the claim based on the following evidence.\n",
            "\n",
            "Claim: A 29-year-old San Diego Zoo intern was arrested and charged with attempting sexually assault a gorilla.\n",
            "\n",
            "Question 0: Where was the gorilla being held?\n",
            "Question 1: Where was the San Diego Zoo intern charged with?\n",
            "Question 2: Was #2 located within the San Diego Zoo?\n",
            "\n",
            "Evidences:\n",
            "\n",
            "1. a gorilla in the kano state zoological gardens has been accused of swallowing n6.8 million. according to bbc pidgin, a radio station in kano, freedom radio claimed that one of the finance officers said that a gorilla swallowed the missing money.\n",
            "2. ... intern in the zoo of san diego. he has been formally accused of repeated sexual assault on a 15 years old male gorilla named big george which is one of the ...\n",
            "3. dec 13, 2020  san diego, california | a san diego zoo employee has been formally charged with repeated sexual assault on a 15-year-old male gorilla.\n",
            "4. ... san diego zoo employee has been formally accused of repeated sexual assault on a 15-year-old male gorilla. akimbo obwe'bwe, a 29-year-old zoo intern ...\n",
            "5. san diego, ca | a san diego zoo employee has been formally accused of repeated sexual assault on a 15-year-old male gorilla.\n",
            "\n",
            "Label (TRUE, FALSE, CONFLICTING):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Prepare training"
      ],
      "metadata": {
        "id": "UGRkeyco9Wyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "metric = load_metric(\"f1\")\n",
        "\n",
        "def to_number(label):\n",
        "  match label:\n",
        "    case \"TRUE\":\n",
        "      return 0\n",
        "    case \"FALSE\":\n",
        "      return 1\n",
        "    case \"CONFLICTING\":\n",
        "      return 2\n",
        "    case _:\n",
        "      return 3\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    decoded_preds = [to_number(pred) for pred in decoded_preds]\n",
        "    decoded_labels = [to_number(label) for label in decoded_labels]\n",
        "\n",
        "    f1_weighted = round(metric.compute(predictions=decoded_preds, references=decoded_labels, average=\"weighted\")['f1'], 4)\n",
        "    f1_macro = round(metric.compute(predictions=decoded_preds, references=decoded_labels, average=\"macro\")['f1'], 4)\n",
        "    return {\"f1_weighted\": f1_weighted, \"f1_macro\": f1_macro}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "cd4754415aff4591b1eab9aee3d1f8ba",
            "348144356e154e01a0f789407ab67d42",
            "b09735fc4bff4802b20f4d662a488f7a",
            "c20cb59c6fdf45878635099c5652ba5c",
            "f901b5dcd5f940e3b3e6e777c74790a6",
            "004f200aa5a548d4b3073cb6358b4e1d",
            "33569b95067f41fdadf13e09333a4937",
            "812e1da4636e44b49882b43e3c1a0f8b",
            "d9ff20d748fc40938e39136a87f88d60",
            "aef22deb20b941349b47bcc4da7fdd5b",
            "d9d5ae0688fc46808c04dca7550cdfef"
          ]
        },
        "id": "015FhZKWHO4l",
        "outputId": "2f697699-509c-42c7-997a-3be5618b5671"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-70406d3cef66>:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"f1\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd4754415aff4591b1eab9aee3d1f8ba"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for f1 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/f1.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
        "import torch\n",
        "\n",
        "# Step 4: Set up training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    predict_with_generate=True,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# Step 5: Initialize the Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_validation,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfoLQCJADvz-",
        "outputId": "94a42547-7c50-4d54-accd-a85c43a33c2d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Train"
      ],
      "metadata": {
        "id": "tZdXN7j38Et5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "6p91R4hM8C8a",
        "outputId": "9fc439ce-bb8c-4522-96a6-84375c884414"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [675/675 15:12, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Weighted</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.182474</td>\n",
              "      <td>0.563400</td>\n",
              "      <td>0.272200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.066878</td>\n",
              "      <td>0.563400</td>\n",
              "      <td>0.272200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.988100</td>\n",
              "      <td>0.057800</td>\n",
              "      <td>0.563400</td>\n",
              "      <td>0.272200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=675, training_loss=2.9841685683638963, metrics={'train_runtime': 915.1933, 'train_samples_per_second': 2.95, 'train_steps_per_second': 0.738, 'total_flos': 6222873467289600.0, 'train_loss': 2.9841685683638963, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/My Drive/t5_large_finetuned_4_epochs\")"
      ],
      "metadata": {
        "id": "Z1IFJVWSiaTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Eval"
      ],
      "metadata": {
        "id": "yUuirsks9euJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained('/content/drive/My Drive/big_boi')"
      ],
      "metadata": {
        "id": "eamdsMd4vMN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation results: {eval_results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "K8YYXIBrETU_",
        "outputId": "bafaa8da-341f-4bf3-8217-ce163dafe922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='771' max='771' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [771/771 04:37]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 38.46904754638672, 'eval_f1_weighted': 0.3205, 'eval_f1_macro': 0.2641, 'eval_runtime': 288.144, 'eval_samples_per_second': 10.703, 'eval_steps_per_second': 2.676}\n"
          ]
        }
      ]
    }
  ]
}