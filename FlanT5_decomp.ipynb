{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "# Use cuda if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Using a generative model with custom prompts depending on claim type\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      4\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_claims_quantemp.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m val_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_claims_quantemp.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\datasets\\__init__.py:43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m pyarrow\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m version\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[1;32mc:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfApi, HfFolder\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pool\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\huggingface_hub\\__init__.py:503\u001b[0m, in \u001b[0;36m_attach.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m attr_to_modules:\n\u001b[0;32m    502\u001b[0m     submod_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 503\u001b[0m     submod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(submod_path)\n\u001b[0;32m    504\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(submod, name)\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# If the attribute lives in a file (module) with the same\u001b[39;00m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001b[39;00m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;66;03m# the module is accessible on the package.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32mc:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\huggingface_hub\\hf_api.py:47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;28;01mas\u001b[39;00m base_tqdm\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_commit_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     51\u001b[0m     CommitOperation,\n\u001b[0;32m     52\u001b[0m     CommitOperationAdd,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     _warn_on_overwriting_operations,\n\u001b[0;32m     60\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1026\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1148\u001b[0m, in \u001b[0;36mpath_stats\u001b[1;34m(self, path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "train_df = pd.read_json(\"train_claims_quantemp.json\")\n",
    "val_df = pd.read_json(\"val_claims_quantemp.json\")\n",
    "\n",
    "train_df = pd.DataFrame([{'type': item['taxonomy_label'], 'claim': item['claim']} for item in train_df.to_dict(orient='records')])\n",
    "val_df = pd.DataFrame([{'type': item['taxonomy_label'], 'claim': item['claim']} for item in val_df.to_dict(orient='records')])\n",
    "\n",
    "# The following code is inspired by ProgramFC and ClaimDecomp\n",
    "# This is a generative model that uses a custom prompt for each claim type\n",
    "# sources: https://github.com/mbzuai-nlp/ProgramFC/blob/main/models/prompts.py\n",
    "# https://arxiv.org/abs/2305.11859\n",
    "# https://aclanthology.org/2023.acl-long.386.pdf\n",
    "\n",
    "main_prompt = '''The task is to break down a given claim into its constituent statements. Here are some examples to illustrate the logic:\n",
    "\n",
    "Example 1:\n",
    "Claim:\n",
    "\"A patent for coronavirus was granted in 2018 to the Pirbright Institute UK, founded by Bill and Melinda Gates.\"\n",
    "Results: \n",
    "1) \"A patent for coronavirus was granted in 2018.\"\n",
    "2) \"The patent was granted to the Pirbright Institute UK.\"\n",
    "3) \"The Pirbright Institute UK was founded by Bill and Melinda Gates.\"\n",
    "\n",
    "Example 2:\n",
    "Claim:\n",
    "\"A video that went viral in July 2023 authentically depicted an ad in Japan that read \\\"Stop Zelenskyy, Stop War.\\\"\" \n",
    "Results: \n",
    "1) \"A video showed an ad in Japan that read \\\"Stop Zelenskyy, Stop War.\\\"\"\n",
    "2) \"The video went viral in July 2023.\"\n",
    "3) \"The video was authentic.\"\n",
    "\n",
    "Example 3:\n",
    "Claim:\n",
    "\"In the historic 2020 US election, Joe Biden defeated Donald Trump to become the 46th President of the United States.\"\n",
    "Results:\n",
    "1) \"The 2020 US election was historic.\"\n",
    "2) \"Joe Biden defeated Donald Trump in the 2020 US election.\"\n",
    "3) \"Joe Biden became the 46th President of the United States.\"\n",
    "\n",
    "Example 4:\n",
    "Claim:\n",
    "\"Arnold Schwarzenegger, born in 1947 in Austria, served as Republican Governor of California after a career as a bodybuilder and actor.\"\n",
    "Results: \n",
    "1) \"Arnold Schwarzenegger was born in 1947 in Austria.\"\n",
    "2) \"Arnold Schwarzenegger served as Republican Governor of California.\"\n",
    "3) \"Arnold Schwarzenegger had a career as a bodybuilder and actor.\"\n",
    "4) \"Arnold Schwarzenegger's career as a bodybuilder and actor preceded his tenure as Governor of California.\"\n",
    "\n",
    "Example 19:\n",
    "Claim:\n",
    "\"According to a 2023 report, the number of billionaires in the world has increased by 25%% since 2020.\"\n",
    "Results:\n",
    "1) \"A report was published in 2023 on the number of billionaires in the world.\"\n",
    "2) \"The number of billionaires in the world has increased between 2020 and 2023.\"\n",
    "3) \"The increase in the number of billionaires is by 25%%.\"\n",
    "\n",
    "\n",
    "Example 17:\n",
    "Claim:\n",
    "\"Japanese citizens boycotting goods made in USA over nuclear bombing in 1945\"\n",
    "Results:\n",
    "1) \"Japanese citizens are boycotting goods made in the USA.\"\n",
    "2) \"There was a nuclear bombing in 1945.\"\n",
    "3) \"The boycott is due to the nuclear bombing in 1945.\"\n",
    "\n",
    "'''\n",
    "\n",
    "# Change the prompt with more specific examples based on the claim type\n",
    "def add_decomp_prompt(claim, claim_type):\n",
    "    if claim_type == \"comparison\":\n",
    "        claim =  main_prompt + f'''\n",
    "Example 13:\n",
    "Claim: \"Pepsi is preferred to Coke in blind taste tests, despite Coke being regarded as the more successful brand.\"\n",
    "Results: \n",
    "1) \"Blind tests have been conducted comparing Pepsi and Coke.\"\n",
    "2) \"Pepsi is preferred to Coke in blind taste tests.\"\n",
    "3) \"Coke is regarded as the more successful brand.\"\n",
    "\n",
    "Example 10:\n",
    "Claim:\n",
    "\"Studies have shown that the average global temperature has increased by 1.2 degrees Celsius since the pre-industrial era.\"\n",
    "Results:\n",
    "1) \"Studies have shown that the average global temperature has increased.\"\n",
    "2) \"The increase is by 1.2 degrees Celsius.\"\n",
    "3) \"The increase is since the pre-industrial era.\"\n",
    "\n",
    "Example 11:\n",
    "Claim:\n",
    "\"According to recent polls, more Americans support the legalization of marijuana than oppose it. This is a significant shift from previous years.\"\n",
    "Results:\n",
    "1) \"Recent polls show that the majority of Americans support the legalization of marijuana.\"\n",
    "2) \"More Americans support the legalization of marijuana than oppose it.\"\n",
    "3) \"There has been a significant shift in public opinion from previous years.\"\n",
    "\n",
    "---\n",
    "Now, break down the following comparison claim into its smallest, factual components. Ensure you list all relevant components and avoid adding unrelated information:\n",
    "\n",
    "Claim: {claim} \n",
    "Results:\n",
    "        '''\n",
    "    elif claim_type == \"interval\":\n",
    "        claim =  main_prompt + f'''\n",
    "Example 15:\n",
    "Claim:\n",
    "\"An image of a red sky in Beijing was taken on March 23, 2023, during a sandstorm.\"\n",
    "Results: \n",
    "1) \"An image of a red sky in Beijing was taken on March 23, 2023.\"\n",
    "2) \"There was a sandstorm in Beijing on March 23, 2023.\"\n",
    "3) \"The image was taken during a sandstorm.\" \n",
    "\n",
    "Example 8:\n",
    "Claim:\n",
    "\"The stock market crashed in 1929, leading to the Great Depression.\"\n",
    "Results:\n",
    "1) \"The stock market crashed in 1929.\"\n",
    "2) \"The stock market crash led to the Great Depression.\"\n",
    "3) \"The Great Depression followed the stock market crash of 1929.\"\n",
    "\n",
    "---\n",
    "Now, break down the following interval claim into its smallest, factual components. Ensure you list all relevant components and avoid adding unrelated information:\n",
    "\n",
    "Claim: {claim} \n",
    "Results:\n",
    "'''  \n",
    "    elif claim_type == \"statistical\":\n",
    "        claim =  main_prompt + f'''\n",
    "\n",
    "Example 6:\n",
    "Claim:\n",
    "\"Police-recorded crimes against property in the EU increased in 2022: thefts rose by 17.9%%, robberies by 9.7%% and burglaries by 7.4%% compared with the previous year.\"\n",
    "Results:\n",
    "1) \"Police-recorded crimes against property in the EU increased in 2022.\"\n",
    "2) \"Thefts rose by 17.9%% compared with the previous year\"\n",
    "3) \"Robberies rose by 9.7%% compared with the previous year\"\n",
    "4) \"Burglaries rose by 7.4%% ccompared with the previous year\"\n",
    "\n",
    "Example 12:\n",
    "Claim:\n",
    "\"President Bolsonaro is facing criticism for deforestation in the Amazon, which has increased by 25%% since he took office.\"\n",
    "Results:\n",
    "1) \"President Bolsonaro is facing criticism for deforestation in the Amazon.\"\n",
    "2) \"Deforestation in the Amazon has increased by 25%%.\"\n",
    "3) \"The increase in deforestation is since President Bolsonaro took office.\"\n",
    "\n",
    "---\n",
    "Now, break down the following statistical claim into its smallest, factual components. Ensure you list all relevant components and avoid adding unrelated information:\n",
    "\n",
    "Claim: {claim} \n",
    "Results:\n",
    "'''\n",
    "    elif claim_type == \"temporal\":\n",
    "        claim =  main_prompt + f'''\n",
    "Example 5:\n",
    "Claim:\n",
    "\"In 2005, an estimated 1.5 million people from Alabama, Mississippi, and Louisiana fled their homes in the face of Hurricane Katrina.\"\n",
    "Results:\n",
    "1) \"In 2005, Hurricane Katrina struck Alabama, Mississippi, and Louisiana.\"\n",
    "2) \"At least 1.5 million fled their homes in the face of Hurricane Katrina.\"\n",
    "3) \"An estimated 1.5 million people who fled their homes were from Alabama, Mississippi, and Louisiana.\"\n",
    "\n",
    "Example 3:\n",
    "Claim:\n",
    "\"The 2022 Winter Olympics in Beijing were the first to feature a unified Korean team.\"\n",
    "Results: \n",
    "1) \"The 2022 Winter Olympics were held in Beijing.\"\n",
    "2) \"The 2022 Winter Olympics featured a unified Korean team.\"\n",
    "3) \"The 2022 unified Korean team was the first in Olympic history.\"\n",
    "\n",
    "---\n",
    "Now, break down the following temporal claim into its smallest, factual components. Ensure you list all relevant components and avoid adding unrelated information:\n",
    "\n",
    "Claim: {claim} \n",
    "Results:\n",
    "'''\n",
    "    else:\n",
    "        claim = f\"Decompose the following claim: {claim} \"\n",
    "    return claim  \n",
    "\n",
    "def planner(df):\n",
    "    # Apply the prompt to each claim and save in separate column\n",
    "    df['prompt'] = df.progress_apply(lambda x: add_decomp_prompt(x['claim'], x['type']), axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = planner(train_df)\n",
    "val_df = planner(val_df)\n",
    "\n",
    "print(train_df['prompt'].head()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Generating Decompositions:   0%|          | 0/312 [00:00<?, ?it/s]c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Generating Decompositions: 100%|██████████| 312/312 [31:40<00:00,  6.09s/it]\n",
      "Generating Decompositions: 100%|██████████| 386/386 [38:00<00:00,  5.91s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "train_tokenized = tokenizer(train_df['prompt'].tolist(), padding=True, max_length=1024, truncation=False, return_tensors=\"pt\")\n",
    "val_tokenized = tokenizer(val_df['prompt'].tolist(), padding=True, max_length=1024, truncation=False, return_tensors=\"pt\")\n",
    "\n",
    "# Decompose claims after tokenization\n",
    "def decompose_claims(input_ids, attention_mask, batch_size = 8):\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, input_ids.size(0), batch_size), desc=\"Generating Decompositions\"):\n",
    "            \n",
    "            # Using pin memory for minor increase in data transfer speed to GPU\n",
    "            batch_input_ids = input_ids[i:i+batch_size].pin_memory().to(device, non_blocking=True)\n",
    "            batch_attention_mask = attention_mask[i:i+batch_size].pin_memory().to(device, non_blocking=True)\n",
    "            outputs = model.generate(\n",
    "                input_ids=batch_input_ids,\n",
    "                attention_mask=batch_attention_mask,\n",
    "                max_length=512,\n",
    "                no_repeat_ngram_size=10,\n",
    "                repetition_penalty=2.5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            for output in outputs:\n",
    "                decomposed_claim = tokenizer.decode(output, skip_special_tokens=True)\n",
    "                results.append(decomposed_claim)\n",
    "    return results\n",
    "\n",
    "train_outputs = decompose_claims(train_tokenized.input_ids, train_tokenized.attention_mask)\n",
    "val_outputs = decompose_claims(val_tokenized.input_ids, val_tokenized.attention_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          type                                              claim  \\\n",
      "0  statistical  \"The non-partisan Congressional Budget Office ...   \n",
      "1  statistical  \"More than 50 percent of immigrants from (El S...   \n",
      "2     temporal  UK government banned Covid vaccine for childre...   \n",
      "3  statistical  \"[In 2014-2015] coverage for the rotavirus vac...   \n",
      "4     temporal  In September 2021, the U.K. government announc...   \n",
      "\n",
      "                                              prompt  \n",
      "0  The task is to break down a given claim into i...  \n",
      "1  The task is to break down a given claim into i...  \n",
      "2  The task is to break down a given claim into i...  \n",
      "3  The task is to break down a given claim into i...  \n",
      "4  The task is to break down a given claim into i...  \n",
      "          type                                              claim  \\\n",
      "0     interval  Amit Shah said Narendra Modi sleeps for 24 hou...   \n",
      "1     temporal  Video of show Pakistani players celebrating th...   \n",
      "2  statistical  Says Dino Rossi \"stripped\" health care \"from 4...   \n",
      "3     interval  Durch einen Vergleich mit den Symptomen einer ...   \n",
      "4  statistical  A gun-toting Australian granny blew the testic...   \n",
      "\n",
      "                                              prompt  \n",
      "0  The task is to break down a given claim into i...  \n",
      "1  The task is to break down a given claim into i...  \n",
      "2  The task is to break down a given claim into i...  \n",
      "3  The task is to break down a given claim into i...  \n",
      "4  The task is to break down a given claim into i...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save results to CSV\n",
    "train_outputs = pd.DataFrame(train_outputs, columns=[\"decomposed_claim\"])\n",
    "val_outputs = pd.DataFrame(val_outputs, columns=[\"decomposed_claim\"])\n",
    "\n",
    "train_outputs.to_csv(\"train_decompositions.csv\", index=False)\n",
    "val_outputs.to_csv(\"val_decompositions.csv\", index=False)\n",
    "\n",
    "# Print sample output\n",
    "print(train_df.head())\n",
    "print(val_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_formatted = pd.read_csv(\"train_decompositions.csv\")\n",
    "val_formatted = pd.read_csv(\"val_decompositions.csv\")\n",
    "\n",
    "# Remove claim number from decomposed claims and fix formatting\n",
    "def format_claims(df):\n",
    "    for i in range(1, 10):\n",
    "        df['decomposed_claim'] = df['decomposed_claim'].str.replace(f'{i}) \"', '\"')\n",
    "    df['decomposed_claim'] = df['decomposed_claim'].str.replace('\")', '\"')\n",
    "    return df\n",
    "\n",
    "format_claims(train_formatted)\n",
    "format_claims(val_formatted)\n",
    "\n",
    "train_formatted.to_csv(\"train_decompositions.csv\", index=False)\n",
    "val_formatted.to_csv(\"val_decompositions.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
