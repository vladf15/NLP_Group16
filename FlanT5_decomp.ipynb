{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "# Use cuda if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Using a generative model with custom prompts depending on claim type\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2495/2495 [00:00<00:00, 35644.11it/s]\n",
      "100%|██████████| 3084/3084 [00:00<00:00, 21055.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The task is to break down a given claim into its constituent statements. Here are some examples to illustrate the logic:\n",
      "\n",
      "Example 1:\n",
      "Claim:\n",
      "\"A patent for coronavirus was granted in 2018 to the Pirbright Institute UK, founded by Bill and Melinda Gates.\"\n",
      "Results: \n",
      "1. \"A patent for coronavirus was granted in 2018.\"\n",
      "\n",
      "2. \"The patent was granted to the Pirbright Institute UK.\"\n",
      "\n",
      "3. \"The Pirbright Institute UK was founded by Bill and Melinda Gates.\"\n",
      "\n",
      "\n",
      "Example 2:\n",
      "Claim:\n",
      "\"A video that went viral in July 2023 authentically depicted an ad in Japan that read \"Stop Zelenskyy, Stop War.\"\" \n",
      "Results: \n",
      "1. \"A video showed an ad in Japan that read \"Stop Zelenskyy, Stop War.\"\"\n",
      "\n",
      "2. \"The video went viral in July 2023.\"\n",
      "\n",
      "3. \"The video was authentic.\"\n",
      "\n",
      "\n",
      "Example 4:\n",
      "Claim:\n",
      "\"Arnold Schwarzenegger, born in 1947 in Austria, served as Republican Governor of California after a career as a bodybuilder and actor.\"\n",
      "Results: \n",
      "1. \"Arnold Schwarzenegger was born in 1947 in Austria.\"\n",
      "\n",
      "2. \"Arnold Schwarzenegger served as Republican Governor of California.\"\n",
      "\n",
      "3. \"Arnold Schwarzenegger had a career as a bodybuilder and actor.\"\n",
      "\n",
      "4. \"Arnold Schwarzenegger's career as a bodybuilder and actor preceded his tenure as Governor of California.\"\n",
      "\n",
      "\n",
      "Example 7:\n",
      "Claim: \n",
      "\"Switching to a plant-based diet can help reduce the risk of heart disease, diabetes, and cancer by up to 50%%.\"\n",
      "Results:\n",
      "1. \"Switching to a plant-based diet can help reduce the risk of heart disease by up to 50%%.\"\n",
      "\n",
      "2. \"Switching to a plant-based diet can help reduce the risk of diabetes by up to 50%%.\"\n",
      "\n",
      "3. \"Switching to a plant-based diet can help reduce the risk of cancer by up to 50%%.\"\n",
      "\n",
      "\n",
      "\n",
      "        \n",
      "        Example 9:\n",
      "        Claim:\n",
      "        \"Vaccines have been shown to reduce the risk of severe COVID-19 by 90%%.\"\n",
      "        Results:\n",
      "        1. \"Vaccines have been shown to reduce the risk of severe COVID-19.\"\n",
      "\n",
      "        2. \"The reduction in risk is by 90%%.\"\n",
      "\n",
      "        3. \"The reduction in risk is for severe COVID-19.\"\n",
      "\n",
      "        \n",
      "        Example 6:\n",
      "        Claim:\n",
      "        \"Police-recorded crimes against property in the EU increased in 2022: thefts rose by 17.9%%, robberies by 9.7%% and burglaries by 7.4%% compared with the previous year.\"\n",
      "        Results:\n",
      "        1. \"Police-recorded crimes against property in the EU increased in 2022.\"\n",
      "\n",
      "        2. \"Thefts rose by 17.9%% compared to 2021.\"\n",
      "\n",
      "        3. \"Robberies rose by 9.7%% compared to 2021.\"\n",
      "\n",
      "        4. \"Burglaries rose by 7.4%% compared to 2021.\"\n",
      "\n",
      "        \n",
      "        Example 12:\n",
      "        Claim:\n",
      "        \"President Bolsonaro is facing criticism for deforestation in the Amazon, which has increased by 25%% since he took office.\"\n",
      "        Results:\n",
      "        1. \"President Bolsonaro is facing criticism for deforestation in the Amazon.\"\n",
      "\n",
      "        2. \"Deforestation in the Amazon has increased by 25%%.\"\n",
      "\n",
      "        3. \"The increase in deforestation is since President Bolsonaro took office.\"\n",
      "\n",
      "\n",
      "        ---\n",
      "        Now, break down the following statistical claim into its smallest components: \n",
      "        \n",
      "        Claim: \"The non-partisan Congressional Budget Office concluded ObamaCare will cost the U.S. more than 800,000 jobs.\" \n",
      "        Results:\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "train_df = pd.read_json(\"test_claims_quantemp.json\")\n",
    "val_df = pd.read_json(\"val_claims_quantemp.json\")\n",
    "\n",
    "train_df = pd.DataFrame([{'type': item['taxonomy_label'], 'claim': item['claim']} for item in train_df.to_dict(orient='records')])\n",
    "val_df = pd.DataFrame([{'type': item['taxonomy_label'], 'claim': item['claim']} for item in val_df.to_dict(orient='records')])\n",
    "\n",
    "# The following code is inspired by the ProgramFC project. We have implemented a simplified version that generates prompts based on task type.\n",
    "# sources: https://github.com/mbzuai-nlp/ProgramFC/blob/main/models/prompts.py\n",
    "# https://aclanthology.org/2023.acl-long.386.pdf\n",
    "# used copilot to assist in generating the prompts\n",
    "\n",
    "\n",
    "main_prompt = '''The task is to break down a given claim into its constituent statements. Here are some examples to illustrate the logic:\n",
    "\n",
    "Example 1:\n",
    "Claim:\n",
    "\"A patent for coronavirus was granted in 2018 to the Pirbright Institute UK, founded by Bill and Melinda Gates.\"\n",
    "Results: \n",
    "1. \"A patent for coronavirus was granted in 2018.\"\\n\n",
    "2. \"The patent was granted to the Pirbright Institute UK.\"\\n\n",
    "3. \"The Pirbright Institute UK was founded by Bill and Melinda Gates.\"\\n\n",
    "\n",
    "Example 2:\n",
    "Claim:\n",
    "\"A video that went viral in July 2023 authentically depicted an ad in Japan that read \\\"Stop Zelenskyy, Stop War.\\\"\" \n",
    "Results: \n",
    "1. \"A video showed an ad in Japan that read \\\"Stop Zelenskyy, Stop War.\\\"\"\\n\n",
    "2. \"The video went viral in July 2023.\"\\n\n",
    "3. \"The video was authentic.\"\\n\n",
    "\n",
    "Example 4:\n",
    "Claim:\n",
    "\"Arnold Schwarzenegger, born in 1947 in Austria, served as Republican Governor of California after a career as a bodybuilder and actor.\"\n",
    "Results: \n",
    "1. \"Arnold Schwarzenegger was born in 1947 in Austria.\"\\n\n",
    "2. \"Arnold Schwarzenegger served as Republican Governor of California.\"\\n\n",
    "3. \"Arnold Schwarzenegger had a career as a bodybuilder and actor.\"\\n\n",
    "4. \"Arnold Schwarzenegger's career as a bodybuilder and actor preceded his tenure as Governor of California.\"\\n\n",
    "\n",
    "Example 7:\n",
    "Claim: \n",
    "\"Switching to a plant-based diet can help reduce the risk of heart disease, diabetes, and cancer by up to 50%%.\"\n",
    "Results:\n",
    "1. \"Switching to a plant-based diet can help reduce the risk of heart disease by up to 50%%.\"\\n\n",
    "2. \"Switching to a plant-based diet can help reduce the risk of diabetes by up to 50%%.\"\\n\n",
    "3. \"Switching to a plant-based diet can help reduce the risk of cancer by up to 50%%.\"\\n\n",
    "\n",
    "'''\n",
    "\n",
    "def add_decomp_prompt(claim, claim_type):\n",
    "    if claim_type == \"comparison\":\n",
    "        claim =  main_prompt + f'''\n",
    "        Example 13:\n",
    "        Claim: \"Pepsi is preferred to Coke in blind taste tests, despite Coke being regarded as the more successful brand.\"\n",
    "        Results: \n",
    "        1. \"Blind tests have been conducted comparing Pepsi and Coke.\"\\n\n",
    "        2. \"Pepsi is preferred to Coke in blind taste tests.\"\\n\n",
    "        3. \"Coke is regarded as the more successful brand.\"\\n\n",
    "        \n",
    "        Example 10:\n",
    "        Claim:\n",
    "        \"Studies have shown that the average global temperature has increased by 1.2 degrees Celsius since the pre-industrial era.\"\n",
    "        Results:\n",
    "        1. \"Studies have shown that the average global temperature has increased.\"\\n\n",
    "        2. \"The increase is by 1.2 degrees Celsius.\"\\n\n",
    "        3. \"The increase is since the pre-industrial era.\"\\n\n",
    "        \n",
    "        Example 11:\n",
    "        Claim:\n",
    "        \"According to recent polls, more Americans support the legalization of marijuana than oppose it. This is a significant shift from previous years.\"\n",
    "        Results:\n",
    "        1. \"There have been recent polls on the legalization of marijuana.\"\\n\n",
    "        2. \"Recent polls show that the majority of Americans support the legalization of marijuana.\"\\n\n",
    "        3. \"The majority of Americans did not support the legalization of marijuana in previous years.\"\\n\n",
    "        \n",
    "        ---\n",
    "        Now, break down the following comparison claim into its smallest components, ensuring there are no duplicates: \n",
    "        \n",
    "        Claim: {claim} \n",
    "        Results:\n",
    "        '''\n",
    "    elif claim_type == \"interval\":\n",
    "        claim =  main_prompt + f'''\n",
    "        Example 15:\n",
    "        Claim:\n",
    "        \"An image of a red sky in Beijing was taken on March 23, 2023, during a sandstorm.\"\\n\n",
    "        Results: \n",
    "        1. \"An image of a red sky in Beijing was taken on March 23, 2023.\"\\n\n",
    "        2. \"There was a sandstorm in Beijing on March 23, 2023.\"\\n\n",
    "        3. \"The image was taken during a sandstorm.\" \\n\n",
    "        \n",
    "        Example 8:\n",
    "        Claim:\n",
    "        \"The stock market crashed in 1929, leading to the Great Depression.\"\n",
    "        Results:\n",
    "        1. \"The stock market crashed in 1929.\"\\n\n",
    "        2. \"The stock market crash led to the Great Depression.\"\\n\n",
    "        3. \"The Great Depression followed the stock market crash of 1929.\"\\n\n",
    "\n",
    "        ---\n",
    "        Now, break down the following interval claim into its smallest components: \n",
    "        \n",
    "        Claim: {claim} \n",
    "        Results:\n",
    "        '''  \n",
    "    elif claim_type == \"statistical\":\n",
    "        claim =  main_prompt + f'''\n",
    "        \n",
    "        Example 9:\n",
    "        Claim:\n",
    "        \"Vaccines have been shown to reduce the risk of severe COVID-19 by 90%%.\"\n",
    "        Results:\n",
    "        1. \"Vaccines have been shown to reduce the risk of severe COVID-19.\"\\n\n",
    "        2. \"The reduction in risk is by 90%%.\"\\n\n",
    "        3. \"The reduction in risk is for severe COVID-19.\"\\n\n",
    "        \n",
    "        Example 6:\n",
    "        Claim:\n",
    "        \"Police-recorded crimes against property in the EU increased in 2022: thefts rose by 17.9%%, robberies by 9.7%% and burglaries by 7.4%% compared with the previous year.\"\n",
    "        Results:\n",
    "        1. \"Police-recorded crimes against property in the EU increased in 2022.\"\\n\n",
    "        2. \"Thefts rose by 17.9%% compared to 2021.\"\\n\n",
    "        3. \"Robberies rose by 9.7%% compared to 2021.\"\\n\n",
    "        4. \"Burglaries rose by 7.4%% compared to 2021.\"\\n\n",
    "        \n",
    "        Example 12:\n",
    "        Claim:\n",
    "        \"President Bolsonaro is facing criticism for deforestation in the Amazon, which has increased by 25%% since he took office.\"\n",
    "        Results:\n",
    "        1. \"President Bolsonaro is facing criticism for deforestation in the Amazon.\"\\n\n",
    "        2. \"Deforestation in the Amazon has increased by 25%%.\"\\n\n",
    "        3. \"The increase in deforestation is since President Bolsonaro took office.\"\\n\n",
    "\n",
    "        ---\n",
    "        Now, break down the following statistical claim into its smallest components: \n",
    "        \n",
    "        Claim: {claim} \n",
    "        Results:\n",
    "        '''\n",
    "    elif claim_type == \"temporal\":\n",
    "        claim =  main_prompt + f'''\n",
    "        Example 5:\n",
    "        Claim:\n",
    "        \"In 2005, an estimated 1.5 million people from Alabama, Mississippi, and Louisiana fled their homes in the face of Hurricane Katrina.\"\n",
    "        Results:\n",
    "        1. \"In 2005, Hurricane Katrina struck Alabama, Mississippi, and Louisiana.\"\\n\n",
    "        2. \"At least 1.5 million fled their homes in the face of Hurricane Katrina.\"\\n\n",
    "        3. \"An estimated 1.5 million people who fled their homes were from Alabama, Mississippi, and Louisiana.\"\\n\n",
    "        \n",
    "        Example 3:\n",
    "        Claim:\n",
    "        \"The 2022 Winter Olympics in Beijing were the first to feature a unified Korean team.\"\n",
    "        Results: \n",
    "        1. \"The 2022 Winter Olympics were held in Beijing.\"\\n\n",
    "        2. \"The 2022 Winter Olympics featured a unified Korean team.\"\\n\n",
    "        3. \"The 2022 unified Korean team was the first in Olympic history.\"\\n\n",
    "        \n",
    "        ---\n",
    "        Now, break down the following temporal claim into its smallest components, ensuring there are no duplicates: \n",
    "        \n",
    "        Claim: {claim} \n",
    "        Results:\n",
    "        '''\n",
    "    else:\n",
    "        claim = f\"Decompose the following claim: {claim} \"\n",
    "    return claim  \n",
    "\n",
    "def planner(df):\n",
    "    # Apply the prompt to each claim and save in separate column\n",
    "    df['prompt'] = df.progress_apply(lambda x: add_decomp_prompt(x['claim'], x['type']), axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = planner(train_df)\n",
    "val_df = planner(val_df)\n",
    "\n",
    "print(train_df['prompt'].head()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Generating Decompositions:   0%|          | 0/2495 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "train_tokenized = tokenizer(train_df['prompt'].tolist(), padding=True, max_length=1024, truncation=False, return_tensors=\"pt\")\n",
    "val_tokenized = tokenizer(val_df['prompt'].tolist(), padding=True, max_length=1024, truncation=False, return_tensors=\"pt\")\n",
    "\n",
    "# Decompose claims after tokenization\n",
    "def decompose_claims(input_ids, attention_mask):\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(input_ids.size(0)), desc=\"Generating Decompositions\"):\n",
    "            outputs = model.generate(input_ids=input_ids[i:i+1].to(device), attention_mask=attention_mask[i:i+1].to(device), \n",
    "                                    max_length=1024,\n",
    "                                    num_beams=5\n",
    "                                    )\n",
    "            decomposed_claim = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            # Print initial claim and decomposition\n",
    "            print(f\"Initial Claim {i+1}/{input_ids.size(0)}: {train_df['claim'][i]}\")\n",
    "            print(f\"Decomposed Claim {i+1}/{input_ids.size(0)}: {decomposed_claim}\")\n",
    "            results.append(decomposed_claim)\n",
    "    return results\n",
    "\n",
    "train_outputs = decompose_claims(train_tokenized.input_ids, train_tokenized.attention_mask)\n",
    "val_outputs = decompose_claims(val_tokenized.input_ids, val_tokenized.attention_mask)\n",
    "\n",
    "# Convert output back to plaintext\n",
    "train_decompositions = [tokenizer.decode(output, skip_special_tokens=True) for output in train_outputs]\n",
    "val_decompositions = [tokenizer.decode(output, skip_special_tokens=True) for output in val_outputs]\n",
    "\n",
    "# Save results to CSV\n",
    "train_df['decomposition'] = train_decompositions\n",
    "val_df['decomposition'] = val_decompositions\n",
    "train_df.to_csv(\"train_decompositions.csv\", index=False)\n",
    "val_df.to_csv(\"val_decompositions.csv\", index=False)\n",
    "\n",
    "# Print sample output\n",
    "print(train_df.head())\n",
    "print(val_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
