{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "# Use cuda if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Using a generative model with custom prompts depending on claim type\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2495/2495 [00:00<00:00, 36694.35it/s]\n",
      "100%|██████████| 3084/3084 [00:00<00:00, 31465.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The task is to break down a given claim into its constituent statements. Here are some examples to illustrate the logic:\n",
      "\n",
      "Example 1:\n",
      "Claim:\n",
      "\"A patent for coronavirus was granted in 2018 to the Pirbright Institute UK, founded by Bill and Melinda Gates.\"\n",
      "Results: \n",
      "1) \"A patent for coronavirus was granted in 2018.\"\n",
      "2) \"The patent was granted to the Pirbright Institute UK.\"\n",
      "3) \"The Pirbright Institute UK was founded by Bill and Melinda Gates.\"\n",
      "\n",
      "Example 2:\n",
      "Claim:\n",
      "\"A video that went viral in July 2023 authentically depicted an ad in Japan that read \"Stop Zelenskyy, Stop War.\"\" \n",
      "Results: \n",
      "1) \"A video showed an ad in Japan that read \"Stop Zelenskyy, Stop War.\"\"\n",
      "2) \"The video went viral in July 2023.\"\n",
      "3) \"The video was authentic.\"\n",
      "\n",
      "Example 4:\n",
      "Claim:\n",
      "\"Arnold Schwarzenegger, born in 1947 in Austria, served as Republican Governor of California after a career as a bodybuilder and actor.\"\n",
      "Results: \n",
      "1) \"Arnold Schwarzenegger was born in 1947 in Austria.\"\n",
      "2) \"Arnold Schwarzenegger served as Republican Governor of California.\"\n",
      "3) \"Arnold Schwarzenegger had a career as a bodybuilder and actor.\"\n",
      "4) \"Arnold Schwarzenegger's career as a bodybuilder and actor preceded his tenure as Governor of California.\"\n",
      "\n",
      "Example 7:\n",
      "Claim: \n",
      "\"Switching to a plant-based diet can help reduce the risk of heart disease, diabetes, and cancer by up to 50%%.\"\n",
      "Results:\n",
      "1) \"Switching to a plant-based diet can help reduce the risk of heart disease by up to 50%%.\"\n",
      "2) \"Switching to a plant-based diet can help reduce the risk of diabetes by up to 50%%.\"\n",
      "3) \"Switching to a plant-based diet can help reduce the risk of cancer by up to 50%%.\"\n",
      "\n",
      "\n",
      "        \n",
      "        Example 9:\n",
      "        Claim:\n",
      "        \"Vaccines have been shown to reduce the risk of severe COVID-19 by 90%%.\"\n",
      "        Results:\n",
      "        1) \"Vaccines have been shown to reduce the risk of severe COVID-19.\"\n",
      "        2) \"The reduction in risk is by 90%%.\"\n",
      "        3) \"The reduction in risk is for severe COVID-19.\"\n",
      "        \n",
      "        Example 6:\n",
      "        Claim:\n",
      "        \"Police-recorded crimes against property in the EU increased in 2022: thefts rose by 17.9%%, robberies by 9.7%% and burglaries by 7.4%% compared with the previous year.\"\n",
      "        Results:\n",
      "        1) \"Police-recorded crimes against property in the EU increased in 2022.\"\n",
      "        2) \"Thefts rose by 17.9%% compared to 2021.\"\n",
      "        3) \"Robberies rose by 9.7%% compared to 2021.\"\n",
      "        4) \"Burglaries rose by 7.4%% compared to 2021.\"\n",
      "        \n",
      "        Example 12:\n",
      "        Claim:\n",
      "        \"President Bolsonaro is facing criticism for deforestation in the Amazon, which has increased by 25%% since he took office.\"\n",
      "        Results:\n",
      "        1) \"President Bolsonaro is facing criticism for deforestation in the Amazon.\"\n",
      "        2) \"Deforestation in the Amazon has increased by 25%%.\"\n",
      "        3) \"The increase in deforestation is since President Bolsonaro took office.\"\n",
      "\n",
      "        ---\n",
      "        Now, break down the following statistical claim into its smallest components: \n",
      "        \n",
      "        Claim: \"The non-partisan Congressional Budget Office concluded ObamaCare will cost the U.S. more than 800,000 jobs.\" \n",
      "        Results:\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "train_df = pd.read_json(\"test_claims_quantemp.json\")\n",
    "val_df = pd.read_json(\"val_claims_quantemp.json\")\n",
    "\n",
    "train_df = pd.DataFrame([{'type': item['taxonomy_label'], 'claim': item['claim']} for item in train_df.to_dict(orient='records')])\n",
    "val_df = pd.DataFrame([{'type': item['taxonomy_label'], 'claim': item['claim']} for item in val_df.to_dict(orient='records')])\n",
    "\n",
    "# The following code is inspired by the ProgramFC project. We have implemented a simplified version that generates prompts based on task type.\n",
    "# sources: https://github.com/mbzuai-nlp/ProgramFC/blob/main/models/prompts.py\n",
    "# https://aclanthology.org/2023.acl-long.386.pdf\n",
    "\n",
    "main_prompt = '''The task is to break down a given claim into its constituent statements. Here are some examples to illustrate the logic:\n",
    "\n",
    "Example 1:\n",
    "Claim:\n",
    "\"A patent for coronavirus was granted in 2018 to the Pirbright Institute UK, founded by Bill and Melinda Gates.\"\n",
    "Results: \n",
    "1) \"A patent for coronavirus was granted in 2018.\"\n",
    "2) \"The patent was granted to the Pirbright Institute UK.\"\n",
    "3) \"The Pirbright Institute UK was founded by Bill and Melinda Gates.\"\n",
    "\n",
    "Example 2:\n",
    "Claim:\n",
    "\"A video that went viral in July 2023 authentically depicted an ad in Japan that read \\\"Stop Zelenskyy, Stop War.\\\"\" \n",
    "Results: \n",
    "1) \"A video showed an ad in Japan that read \\\"Stop Zelenskyy, Stop War.\\\"\"\n",
    "2) \"The video went viral in July 2023.\"\n",
    "3) \"The video was authentic.\"\n",
    "\n",
    "Example 4:\n",
    "Claim:\n",
    "\"Arnold Schwarzenegger, born in 1947 in Austria, served as Republican Governor of California after a career as a bodybuilder and actor.\"\n",
    "Results: \n",
    "1) \"Arnold Schwarzenegger was born in 1947 in Austria.\"\n",
    "2) \"Arnold Schwarzenegger served as Republican Governor of California.\"\n",
    "3) \"Arnold Schwarzenegger had a career as a bodybuilder and actor.\"\n",
    "4) \"Arnold Schwarzenegger's career as a bodybuilder and actor preceded his tenure as Governor of California.\"\n",
    "\n",
    "Example 7:\n",
    "Claim: \n",
    "\"Switching to a plant-based diet can help reduce the risk of heart disease, diabetes, and cancer by up to 50%%.\"\n",
    "Results:\n",
    "1) \"Switching to a plant-based diet can help reduce the risk of heart disease by up to 50%%.\"\n",
    "2) \"Switching to a plant-based diet can help reduce the risk of diabetes by up to 50%%.\"\n",
    "3) \"Switching to a plant-based diet can help reduce the risk of cancer by up to 50%%.\"\n",
    "\n",
    "'''\n",
    "\n",
    "def add_decomp_prompt(claim, claim_type):\n",
    "    if claim_type == \"comparison\":\n",
    "        claim =  main_prompt + f'''\n",
    "        Example 13:\n",
    "        Claim: \"Pepsi is preferred to Coke in blind taste tests, despite Coke being regarded as the more successful brand.\"\n",
    "        Results: \n",
    "        1) \"Blind tests have been conducted comparing Pepsi and Coke.\"\n",
    "        2) \"Pepsi is preferred to Coke in blind taste tests.\"\n",
    "        3) \"Coke is regarded as the more successful brand.\"\n",
    "        \n",
    "        Example 10:\n",
    "        Claim:\n",
    "        \"Studies have shown that the average global temperature has increased by 1.2 degrees Celsius since the pre-industrial era.\"\n",
    "        Results:\n",
    "        1) \"Studies have shown that the average global temperature has increased.\"\n",
    "        2) \"The increase is by 1.2 degrees Celsius.\"\n",
    "        3) \"The increase is since the pre-industrial era.\"\n",
    "        \n",
    "        Example 11:\n",
    "        Claim:\n",
    "        \"According to recent polls, more Americans support the legalization of marijuana than oppose it. This is a significant shift from previous years.\"\n",
    "        Results:\n",
    "        1) \"There have been recent polls on the legalization of marijuana.\"\n",
    "        2) \"Recent polls show that the majority of Americans support the legalization of marijuana.\"\n",
    "        3) \"The majority of Americans did not support the legalization of marijuana in previous years.\"\n",
    "        \n",
    "        ---\n",
    "        Now, break down the following comparison claim into its smallest components: \n",
    "        \n",
    "        Claim: {claim} \n",
    "        Results:\n",
    "        '''\n",
    "    elif claim_type == \"interval\":\n",
    "        claim =  main_prompt + f'''\n",
    "        Example 15:\n",
    "        Claim:\n",
    "        \"An image of a red sky in Beijing was taken on March 23, 2023, during a sandstorm.\"\n",
    "        Results: \n",
    "        1) \"An image of a red sky in Beijing was taken on March 23, 2023.\"\n",
    "        2) \"There was a sandstorm in Beijing on March 23, 2023.\"\n",
    "        3) \"The image was taken during a sandstorm.\" \n",
    "        \n",
    "        Example 8:\n",
    "        Claim:\n",
    "        \"The stock market crashed in 1929, leading to the Great Depression.\"\n",
    "        Results:\n",
    "        1) \"The stock market crashed in 1929.\"\n",
    "        2) \"The stock market crash led to the Great Depression.\"\n",
    "        3) \"The Great Depression followed the stock market crash of 1929.\"\n",
    "\n",
    "        ---\n",
    "        Now, break down the following interval claim into its smallest components: \n",
    "        \n",
    "        Claim: {claim} \n",
    "        Results:\n",
    "        '''  \n",
    "    elif claim_type == \"statistical\":\n",
    "        claim =  main_prompt + f'''\n",
    "        \n",
    "        Example 9:\n",
    "        Claim:\n",
    "        \"Vaccines have been shown to reduce the risk of severe COVID-19 by 90%%.\"\n",
    "        Results:\n",
    "        1) \"Vaccines have been shown to reduce the risk of severe COVID-19.\"\n",
    "        2) \"The reduction in risk is by 90%%.\"\n",
    "        3) \"The reduction in risk is for severe COVID-19.\"\n",
    "        \n",
    "        Example 6:\n",
    "        Claim:\n",
    "        \"Police-recorded crimes against property in the EU increased in 2022: thefts rose by 17.9%%, robberies by 9.7%% and burglaries by 7.4%% compared with the previous year.\"\n",
    "        Results:\n",
    "        1) \"Police-recorded crimes against property in the EU increased in 2022.\"\n",
    "        2) \"Thefts rose by 17.9%% compared to 2021.\"\n",
    "        3) \"Robberies rose by 9.7%% compared to 2021.\"\n",
    "        4) \"Burglaries rose by 7.4%% compared to 2021.\"\n",
    "        \n",
    "        Example 12:\n",
    "        Claim:\n",
    "        \"President Bolsonaro is facing criticism for deforestation in the Amazon, which has increased by 25%% since he took office.\"\n",
    "        Results:\n",
    "        1) \"President Bolsonaro is facing criticism for deforestation in the Amazon.\"\n",
    "        2) \"Deforestation in the Amazon has increased by 25%%.\"\n",
    "        3) \"The increase in deforestation is since President Bolsonaro took office.\"\n",
    "\n",
    "        ---\n",
    "        Now, break down the following statistical claim into its smallest components: \n",
    "        \n",
    "        Claim: {claim} \n",
    "        Results:\n",
    "        '''\n",
    "    elif claim_type == \"temporal\":\n",
    "        claim =  main_prompt + f'''\n",
    "        Example 5:\n",
    "        Claim:\n",
    "        \"In 2005, an estimated 1.5 million people from Alabama, Mississippi, and Louisiana fled their homes in the face of Hurricane Katrina.\"\n",
    "        Results:\n",
    "        1) \"In 2005, Hurricane Katrina struck Alabama, Mississippi, and Louisiana.\"\n",
    "        2) \"At least 1.5 million fled their homes in the face of Hurricane Katrina.\"\n",
    "        3) \"An estimated 1.5 million people who fled their homes were from Alabama, Mississippi, and Louisiana.\"\n",
    "        \n",
    "        Example 3:\n",
    "        Claim:\n",
    "        \"The 2022 Winter Olympics in Beijing were the first to feature a unified Korean team.\"\n",
    "        Results: \n",
    "        1) \"The 2022 Winter Olympics were held in Beijing.\"\n",
    "        2) \"The 2022 Winter Olympics featured a unified Korean team.\"\n",
    "        3) \"The 2022 unified Korean team was the first in Olympic history.\"\n",
    "        \n",
    "        ---\n",
    "        Now, break down the following temporal claim into its smallest components: \n",
    "        \n",
    "        Claim: {claim} \n",
    "        Results:\n",
    "        '''\n",
    "    else:\n",
    "        claim = f\"Decompose the following claim: {claim} \"\n",
    "    return claim  \n",
    "\n",
    "def planner(df):\n",
    "    # Apply the prompt to each claim and save in separate column\n",
    "    df['prompt'] = df.progress_apply(lambda x: add_decomp_prompt(x['claim'], x['type']), axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = planner(train_df)\n",
    "val_df = planner(val_df)\n",
    "\n",
    "print(train_df['prompt'].head()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Generating Decompositions:   0%|          | 0/2495 [00:00<?, ?it/s]c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Generating Decompositions:   0%|          | 1/2495 [00:18<12:52:48, 18.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 1/2495: \"The non-partisan Congressional Budget Office concluded ObamaCare will cost the U.S. more than 800,000 jobs.\"\n",
      "Decomposed Claim 1/2495: <pad> 1) \"The non-partisan Congressional Budget Office concluded ObamaCare will cost the U.S. more than 800,000 jobs.\" 2) \"The non-partisan Congressional Budget Office concluded ObamaCare will cost the U.S. more than 800,000 jobs.\"</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   0%|          | 2/2495 [00:32<10:59:48, 15.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 2/2495: \"More than 50 percent of immigrants from (El Salvador, Guatemala and Honduras) use at least one major welfare program once they get here.\"\n",
      "Decomposed Claim 2/2495: <pad> 1) \"More than 50 percent of immigrants from (El Salvador, Guatemala and Honduras) use at least one major welfare program once they get here.\"</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   0%|          | 3/2495 [00:45<10:02:25, 14.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 3/2495: UK government banned Covid vaccine for children age 5-11\n",
      "Decomposed Claim 3/2495: <pad> 1) \"UK government banned Covid vaccine for children age 5-11.\" 2) \"Covid vaccine is a vaccine that protects against viruses.\"</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   0%|          | 4/2495 [01:03<10:58:29, 15.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 4/2495: \"[In 2014-2015] coverage for the rotavirus vaccine exceeded the 95% target and the pneumococcal vaccine reached 91.5%.\"\n",
      "Decomposed Claim 4/2495: <pad> 1) \"[In 2014-2015] coverage for the rotavirus vaccine exceeded the 95% target.\" 2) \"The pneumococcal vaccine reached 91.5%.\" 3) \"The rotavirus vaccine exceeded the 95% target.\"</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   0%|          | 5/2495 [01:26<12:42:36, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 5/2495: In September 2021, the U.K. government announced its intention to create a new criminal offense of pet abduction.\n",
      "Decomposed Claim 5/2495: <pad> 1) \"The U.K. government announced its intention to create a new criminal offense of pet abduction in September 2021.\" 2) \"The U.K. government announced its intention to create a new criminal offense of pet abduction in September 2021.\" 3) \"The U.K. government announced its intention to create a new criminal offense of pet abduction in September 2021.\"</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   0%|          | 6/2495 [01:38<11:15:54, 16.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 6/2495: Labour took £1.5 million from Just Stop Oil.\n",
      "Decomposed Claim 6/2495: <pad> 1) \"Just Stop Oil\" raised £1.5 million for Labour. 2) \"Labour took £1.5 million from Just Stop Oil.\"</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   0%|          | 7/2495 [01:58<12:00:31, 17.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 7/2495: McDonald's has announced that everyone who shares this link will receive 2 FREE McFamily box\n",
      "Decomposed Claim 7/2495: <pad> 1) \"McDonald's has announced that everyone who shares this link will receive 2 FREE McFamily box.\" 2) \"Everyone who shares this link will receive 2 FREE McFamily box.\" 3) \"Everyone who shares this link will receive 2 FREE McFamily box.\"</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   0%|          | 8/2495 [02:10<10:58:31, 15.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 8/2495: \"NASA just announced a 100-foot-wide fissure-crack just opened up Yellowstone volcano in 24 hours.”\n",
      "Decomposed Claim 8/2495: <pad> 1) \"NASA just announced a 100-foot-wide fissure-crack just opened up Yellowstone volcano in 24 hours.\"</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   0%|          | 9/2495 [02:21<9:54:35, 14.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 9/2495: Photo shows woman watching climate protest outside Ivanka Trump’s house in 2017\n",
      "Decomposed Claim 9/2495: <pad> 1) \"Photo shows woman watching climate protest outside Ivanka Trump’s house in 2017\"</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   0%|          | 10/2495 [02:36<10:03:16, 14.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 10/2495: The 1974 comedy “Young Frankenstein” directly inspired the title for rock band Aerosmith’s song “Walk This Way.”\n",
      "Decomposed Claim 10/2495: <pad> 1) \"Young Frankenstein\" is a 1974 comedy film. 2) \"Walk This Way\" is a song by Aerosmith. 3) \"Young Frankenstein\" is a film.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   0%|          | 11/2495 [02:54<10:46:41, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 11/2495: A total of 58 peer-reviewed papers published in the first half of 2017 conclude that global warming is a myth.\n",
      "Decomposed Claim 11/2495: <pad> 1) \"A total of 58 peer-reviewed papers published in the first half of 2017 conclude that global warming is a myth.\" 2) \"A total of 58 peer-reviewed papers published in the first half of 2017 conclude that global warming is a myth.\"</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   0%|          | 12/2495 [03:05<9:46:29, 14.17s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 12/2495: \"Senator Clinton tried to spend $1 million on the Woodstock Concert Museum.\"\n",
      "Decomposed Claim 12/2495: <pad> 1) \"Senator Clinton tried to spend $1 million on the Woodstock Concert Museum.\"</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   1%|          | 13/2495 [03:21<10:04:12, 14.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 13/2495: \"When you throw 23 million people off of health insurance -- people with cancer, people with heart disease, people with diabetes -- thousands of people will die. … This is study after study making this point.\"\n",
      "Decomposed Claim 13/2495: <pad> 1) \"When you throw 23 million people off of health insurance -- people with cancer, people with heart disease, people with diabetes -- thousands of people will die.\" 2) \"This study after study making this point.\"</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   1%|          | 14/2495 [03:58<14:50:01, 21.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 14/2495: Says Arizona, Missouri and Texas residents have a two-pet limit, so the public must \"surrender their third pet to the Humane Society.\"\n",
      "Decomposed Claim 14/2495: <pad> 1) \"Arizona, Missouri and Texas residents have a two-pet limit, so the public must surrender their third pet to the Humane Society.\" 2) \"Arizona, Missouri and Texas residents have a two-pet limit, so the public must surrender their third pet to the Humane Society.\" 3) \"Arizona, Missouri and Texas residents have a two-pet limit, so the public must surrender their third pet to the Humane Society.\"</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   1%|          | 15/2495 [04:20<14:47:39, 21.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 15/2495: \"We have an unemployment (rate of) 1.5 in Boone County, and we can’t find enough workers.\"\n",
      "Decomposed Claim 15/2495: <pad> 1) \"We have an unemployment rate of 1.5 in Boone County.\" 2) \"We can’t find enough workers.\"</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   1%|          | 16/2495 [04:40<14:32:19, 21.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 16/2495: \"Computer models show Irma destroying New York City on Sept. 10.\"\n",
      "Decomposed Claim 16/2495: <pad> 1) \"Computer models show Irma destroying New York City on Sept. 10.\"</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Decompositions:   1%|          | 17/2495 [05:01<14:31:49, 21.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Claim 17/2495: Antifa activists announced in August 2017 that they would protest the Sturgis Motorcycle Rally in 2018.\n",
      "Decomposed Claim 17/2495: <pad> 1) \"Antifa activists announced in August 2017 that they would protest the Sturgis Motorcycle Rally in 2018.\"</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "train_tokenized = tokenizer(train_df['prompt'].tolist(), padding=True, max_length=1024, truncation=False, return_tensors=\"pt\")\n",
    "val_tokenized = tokenizer(val_df['prompt'].tolist(), padding=True, max_length=1024, truncation=False, return_tensors=\"pt\")\n",
    "\n",
    "# Decompose claims after tokenization\n",
    "def decompose_claims(input_ids, attention_mask):\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(input_ids.size(0)), desc=\"Generating Decompositions\"):\n",
    "            outputs = model.generate(input_ids=input_ids[i:i+1].to(device), attention_mask=attention_mask[i:i+1].to(device), \n",
    "                                    max_length=512,\n",
    "                                    early_stopping=True,\n",
    "                                    no_repeat_ngram_size=20,  \n",
    "                                    )\n",
    "            decomposed_claim = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            # Print initial claim and decomposition\n",
    "            print(f\"Initial Claim {i+1}/{input_ids.size(0)}: {train_df['claim'][i]}\")\n",
    "            print(f\"Decomposed Claim {i+1}/{input_ids.size(0)}: {decomposed_claim}\")\n",
    "            results.append(decomposed_claim)\n",
    "    return results\n",
    "\n",
    "train_outputs = decompose_claims(train_tokenized.input_ids, train_tokenized.attention_mask)\n",
    "val_outputs = decompose_claims(val_tokenized.input_ids, val_tokenized.attention_mask)\n",
    "\n",
    "# Convert output back to plaintext\n",
    "train_decompositions = [tokenizer.decode(output, skip_special_tokens=False) for output in train_outputs]\n",
    "val_decompositions = [tokenizer.decode(output, skip_special_tokens=False) for output in val_outputs]\n",
    "\n",
    "# Save results to CSV\n",
    "train_df['decomposition'] = train_decompositions\n",
    "val_df['decomposition'] = val_decompositions\n",
    "train_df.to_csv(\"train_decompositions.csv\", index=False)\n",
    "val_df.to_csv(\"val_decompositions.csv\", index=False)\n",
    "\n",
    "# Print sample output\n",
    "print(train_df.head())\n",
    "print(val_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
