{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "# Use cuda if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\", num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#in small part inspired by the oracle code\n",
    "#https://colab.research.google.com/drive/1gZJCakmY28cKGMj8B7wd1GUM3r72pdbi?usp=sharing#scrollTo=8eKFjiC3i8Yx\n",
    "# Function to get labels and claims from the dataset\n",
    "\n",
    "train_df = pd.read_json('train_claims_quantemp.json')\n",
    "val_df = pd.read_json('val_claims_quantemp.json')\n",
    "#test_df = pd.read_json('test_claims_quantemp.json')\n",
    "\n",
    "train_df = pd.DataFrame([{'label': item['label'], 'claim': item['claim']} for item in train_df.to_dict(orient='records')])\n",
    "val_df = pd.DataFrame([{'label': item['label'], 'claim': item['claim']} for item in val_df.to_dict(orient='records')])\n",
    "#test_df = pd.DataFrame([{'label': item['label'], 'claim': item['claim']} for item in test_df.to_dict(orient='records')])\n",
    "\n",
    "#old evidence file for bm25\n",
    "#evidence_df = pd.read_json('bm25_top_100_claimdecomp.json')\n",
    "#evidence_df = pd.DataFrame([{'claim': item['claim'], 'docs': item['docs'], 'scores': item['scores']} for item in evidence_df.to_dict(orient='records')])\n",
    "\n",
    "#load csv evidence files for training and validation\n",
    "train_evidence_df = pd.read_csv('NLP_Group16/evidences_train.csv')\n",
    "val_evidence_df = pd.read_csv('NLP_Group16/evidences_val.csv')\n",
    "train_evidence_df = train_evidence_df[['claim', 'evidences', 'scores']]\n",
    "val_evidence_df = val_evidence_df[['claim', 'evidences', 'scores']]\n",
    "\n",
    "\n",
    "# Get top_k relevant evidence for each claim, default is 5\n",
    "def combine_top_k_evidence(row, top_k = 5):\n",
    "    evidences = row['evidences'].strip('[]').split(', ')\n",
    "    scores = list(map(float, row['scores'].strip('[]').split(', ')))\n",
    "    ranked_documents = sorted(zip(evidences, scores), key=lambda x: x[1], reverse=True)\n",
    "    top_k_documents = [doc.strip('\"') for doc, score in ranked_documents[:top_k]]\n",
    "    return ' '.join(top_k_documents)\n",
    "\n",
    "# Apply the function to get top_k evidence for each claim\n",
    "train_evidence_df['top_k_docs'] = train_evidence_df.apply(lambda row: combine_top_k_evidence(row), axis=1)\n",
    "val_evidence_df['top_k_docs'] = val_evidence_df.apply(lambda row: combine_top_k_evidence(row), axis=1)\n",
    "\n",
    "# Drop docs and scores columns as they are no longer needed\n",
    "train_evidence_df = train_evidence_df.drop(columns=['evidences', 'scores'])\n",
    "val_evidence_df = val_evidence_df.drop(columns=['evidences', 'scores'])\n",
    "\n",
    "# Merge the combined evidence with the train, validation, and test DataFrames\n",
    "train_df = train_df.merge(train_evidence_df[['claim', 'top_k_docs']], on='claim', how='left')\n",
    "val_df = val_df.merge(val_evidence_df[['claim', 'top_k_docs']], on='claim', how='left')\n",
    "#test_df = test_df.merge(evidence_df[['claim', 'top_k_docs']], on='claim', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(df, max_length = 256):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        df['claim'].tolist(),\n",
    "        df['top_k_docs'].fillna('').tolist(),  # Fill NaN with empty strings\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = df['label'].apply(lambda x: {'False': 0, 'True': 1, 'Conflicting': 2}[x]).tolist()\n",
    "    return tokenized_inputs, labels\n",
    "\n",
    "train_tokenized, train_labels = tokenize_data(train_df)\n",
    "val_tokenized, val_labels = tokenize_data(val_df)\n",
    "#test_tokenized, test_labels = tokenize_data(test_df)\n",
    "\n",
    "print(train_tokenized['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom Dataset class for the tokenized data\n",
    "class ClaimsDataset(Dataset):\n",
    "    def __init__(self, tokenized_inputs, labels):\n",
    "        self.input_ids = tokenized_inputs['input_ids']\n",
    "        self.attention_mask = tokenized_inputs['attention_mask']\n",
    "        self.labels = torch.tensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[index],\n",
    "            'attention_mask': self.attention_mask[index],\n",
    "            'labels': self.labels[index]\n",
    "        }\n",
    "\n",
    "train_dataset = ClaimsDataset(train_tokenized, train_labels)\n",
    "val_dataset = ClaimsDataset(val_tokenized, val_labels)\n",
    "#test_dataset = ClaimsDataset(test_tokenized, test_labels)\n",
    "\n",
    "train_tokenized = {key: val.to(device) for key, val in train_tokenized.items()}\n",
    "val_tokenized = {key: val.to(device) for key, val in val_tokenized.items()}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers= 4, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers= 4, pin_memory=True)\n",
    "#test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=torch.inf)\n",
    "print(train_evidence_df[:3])\n",
    "\n",
    "# This is just to check if the data is properly processed\n",
    "for i, batch in enumerate(train_dataset):\n",
    "    assert batch['input_ids'].shape == batch['attention_mask'].shape\n",
    "    print(f\"Batch {i+1}\")\n",
    "    print(batch)\n",
    "    if i == 2:  \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback, TrainerState, TrainerControl, AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "output_dir = './results'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "print(f\"Output directory is set to: {output_dir}\")\n",
    "\n",
    "# Function to get metrics from the model predictions\n",
    "def compute_metrics(inputs):\n",
    "    predictions, labels = inputs\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    metrics = {}\n",
    "    # Compute overall accuracy\n",
    "    metrics['accuracy'] = accuracy_score(labels, preds)\n",
    "    # Compute macro F1-score and weighted scores for the rest of the metrics\n",
    "    _, _, f1_macro_overall, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    metrics['f1_macro'] = f1_macro_overall\n",
    "    \n",
    "    precision_overall, recall_overall, f1_weighted_overall, support_overall = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    metrics['f1_weighted'] = f1_weighted_overall\n",
    "    metrics['precision'] = precision_overall\n",
    "    metrics['recall'] = recall_overall\n",
    "    metrics['support'] = support_overall\n",
    "    \n",
    "    # Compute metrics for each label\n",
    "    for label_name, label_id in {'False': 0, 'True': 1, 'Conflicting': 2}.items():\n",
    "        precision, recall, f1_weighted, _ = precision_recall_fscore_support(labels, preds, labels=[label_id], average='weighted')\n",
    "        metrics[f'{label_name}_precision'] = precision\n",
    "        metrics[f'{label_name}_recall'] = recall\n",
    "        metrics[f'{label_name}_f1_weighted'] = f1_weighted    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1,\n",
    "    save_steps = 5000,\n",
    "    use_cpu=False, # Use GPU for training\n",
    "    fp16=True,  # Enable mixed precision training\n",
    ")\n",
    "\n",
    "\n",
    "labels = train_df['label'].apply(lambda x: {'False': 0, 'True': 1, 'Conflicting': 2}[x])\n",
    "label_weights = compute_class_weight('balanced', classes=[0, 1, 2], y=labels)\n",
    "label_weights = torch.tensor(label_weights, dtype=torch.float).to(device)\n",
    "print(f\"Label weights: {label_weights}\")\n",
    "\n",
    "\n",
    "# Logging predictions and labels to check if the model is learning\n",
    "# Used AI tools to help make this function since I wasn't sure of the proper syntax\n",
    "# not really relevant to the results regardless, just so I don't have to run the entire model to see if it's learning\n",
    "class PredictionLoggerCallback(TrainerCallback):\n",
    "   def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % args.logging_steps == 0:\n",
    "            model = kwargs.get('model')\n",
    "            data_loader = kwargs.get('train_dataloader')\n",
    "            \n",
    "            # Show how training is progressing by logging predictions and labels\n",
    "            if model and data_loader:\n",
    "                model.eval()\n",
    "                batch = next(iter(data_loader))\n",
    "                inputs = {key: value.to(args.device) for key, value in batch.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    preds = torch.argmax(logits, axis=-1)\n",
    "                    labels = inputs['labels']\n",
    "                    \n",
    "                    print(f\"Step {state.global_step}:\")\n",
    "                    print(f\"Predictions: {preds.cpu().numpy()}\")\n",
    "                    print(f\"Labels: {labels.cpu().numpy()}\")\n",
    "                model.train()\n",
    "        return control               \n",
    "\n",
    "# Custom trainer class that accounts for imbalances in label occurrences\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = CrossEntropyLoss(weight=label_weights)(logits, labels)\n",
    "        if(return_outputs):\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "log_callback = PredictionLoggerCallback()\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(optimizer, None),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[log_callback]\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(val_dataset)\n",
    "\n",
    "# Writing results + metrics to file\n",
    "results_file = os.path.join(training_args.output_dir, \"evaluation_results.txt\")\n",
    "with open(results_file, \"w\") as writer:\n",
    "    for key, value in results.items():\n",
    "        writer.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Run predictions on the test set to perform qualitative analysis and find failed predictions\n",
    "predictions_output = trainer.predict(val_dataset)\n",
    "predictions = np.argmax(predictions_output.predictions, axis=1)\n",
    "labels = predictions_output.label_ids\n",
    "failed_indices = np.where(predictions != labels)[0]\n",
    "\n",
    "# Extract the corresponding claims and predictions\n",
    "failed_claims = val_df.iloc[failed_indices]\n",
    "failed_predictions = predictions[failed_indices]\n",
    "failed_labels = labels[failed_indices]\n",
    "\n",
    "# Randomly select claims for qualitative analysis get the corresponding labels and predictions\n",
    "num_samples = 20\n",
    "random_indices = random.sample(range(len(failed_indices)), min(num_samples, len(failed_indices)))\n",
    "selected_failed_claims = failed_claims.iloc[random_indices]\n",
    "selected_failed_predictions = failed_predictions[random_indices]\n",
    "selected_failed_labels = failed_labels[random_indices]\n",
    "\n",
    "qualitative_analysis_df = pd.DataFrame({\n",
    "    'claim': selected_failed_claims['claim'].values,\n",
    "    'top_k_docs': selected_failed_claims['top_k_docs'].values,\n",
    "    'real_label': selected_failed_labels,\n",
    "    'predicted_label': selected_failed_predictions\n",
    "})\n",
    "\n",
    "# Map numerical labels to their string representations\n",
    "label_map = {0: 'False', 1: 'True', 2: 'Conflicting'}\n",
    "qualitative_analysis_df['real_label'] = qualitative_analysis_df['real_label'].map(label_map)\n",
    "qualitative_analysis_df['predicted_label'] = qualitative_analysis_df['predicted_label'].map(label_map)\n",
    "\n",
    "# Save the qualitative analysis results to a text file\n",
    "qualitative_analysis_file = os.path.join(training_args.output_dir, \"qualitative_analysis.txt\")\n",
    "with open(qualitative_analysis_file, \"w\") as writer:\n",
    "    for index, row in qualitative_analysis_df.iterrows():\n",
    "        writer.write(f\"Claim: {row['claim']}\\n\")\n",
    "        writer.write(f\"Evidence: {row['top_k_docs']}\\n\")\n",
    "        writer.write(f\"Real Label: {row['real_label']}\\n\")\n",
    "        writer.write(f\"Predicted Label: {row['predicted_label']}\\n\")\n",
    "        writer.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of labels in predictions\n",
    "train_label_counts = pd.Series(train_labels).value_counts()\n",
    "print(train_label_counts)\n",
    "val_label_counts = pd.Series(val_labels).value_counts()\n",
    "print(val_label_counts)\n",
    "prediction_label_counts = pd.Series(predictions).value_counts()\n",
    "print(prediction_label_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
