{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "# Use cuda if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\", num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#inspired by the oracle code\n",
    "#https://colab.research.google.com/drive/1gZJCakmY28cKGMj8B7wd1GUM3r72pdbi?usp=sharing#scrollTo=8eKFjiC3i8Yx\n",
    "# Function to get labels and claims from the dataset\n",
    "\n",
    "train_df = pd.read_json('train_claims_quantemp.json')\n",
    "val_df = pd.read_json('val_claims_quantemp.json')\n",
    "#test_df = pd.read_json('test_claims_quantemp.json')\n",
    "\n",
    "train_df = pd.DataFrame([{'label': item['label'], 'claim': item['claim']} for item in train_df.to_dict(orient='records')])\n",
    "val_df = pd.DataFrame([{'label': item['label'], 'claim': item['claim']} for item in val_df.to_dict(orient='records')])\n",
    "#test_df = pd.DataFrame([{'label': item['label'], 'claim': item['claim']} for item in test_df.to_dict(orient='records')])\n",
    "\n",
    "#old evidence file for bm25\n",
    "#evidence_df = pd.read_json('bm25_top_100_claimdecomp.json')\n",
    "#evidence_df = pd.DataFrame([{'claim': item['claim'], 'docs': item['docs'], 'scores': item['scores']} for item in evidence_df.to_dict(orient='records')])\n",
    "\n",
    "#load csv evidence files for training and validation\n",
    "train_evidence_df = pd.read_csv('NLP_Group16/evidences_train.csv')\n",
    "val_evidence_df = pd.read_csv('NLP_Group16/evidences_val.csv')\n",
    "train_evidence_df = train_evidence_df[['claim', 'evidences', 'scores']]\n",
    "val_evidence_df = val_evidence_df[['claim', 'evidences', 'scores']]\n",
    "\n",
    "\n",
    "# Get top_k relevant evidence for each claim\n",
    "def combine_top_k_evidence(row, top_k):\n",
    "    evidences = row['evidences'].strip('[]').split(', ')\n",
    "    scores = list(map(float, row['scores'].strip('[]').split(', ')))\n",
    "    ranked_documents = sorted(zip(evidences, scores), key=lambda x: x[1], reverse=True)\n",
    "    top_k_documents = [doc.strip('\"') for doc, score in ranked_documents[:top_k]]\n",
    "    return ' '.join(top_k_documents)\n",
    "\n",
    "top_k = 5\n",
    "# Apply the function to get top_k evidence for each claim\n",
    "train_evidence_df['top_k_docs'] = train_evidence_df.apply(lambda row: combine_top_k_evidence(row, top_k), axis=1)\n",
    "val_evidence_df['top_k_docs'] = val_evidence_df.apply(lambda row: combine_top_k_evidence(row, top_k), axis=1)\n",
    "\n",
    "# Drop docs and scores columns as they are no longer needed\n",
    "train_evidence_df = train_evidence_df.drop(columns=['evidences', 'scores'])\n",
    "val_evidence_df = val_evidence_df.drop(columns=['evidences', 'scores'])\n",
    "\n",
    "# Merge the combined evidence with the train, validation, and test DataFrames\n",
    "train_df = train_df.merge(train_evidence_df[['claim', 'top_k_docs']], on='claim', how='left')\n",
    "val_df = val_df.merge(val_evidence_df[['claim', 'top_k_docs']], on='claim', how='left')\n",
    "#test_df = test_df.merge(evidence_df[['claim', 'top_k_docs']], on='claim', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,  1121,    69,  1229,  1901,     6,   234,  9856,  2331, 33922,\n",
      "          271,  7243,  1695,    14,     5,  1621,  7664,  1718,     6,   151,\n",
      "         4963, 10918, 27353,    11,     5,   247,     4,     2,     2,  3865,\n",
      "        27953,    35,   295,  9856,  2331,   579,  3432,   271,  7243,   399,\n",
      "           75,  2026,    14,   213, 26390,  7664,  1718,     6,   151, 11398,\n",
      "         1535,   669, 27353,     4,    31,     5,   569,  9565,     5,  1901,\n",
      "           24,    16,   699,    14,    79,    26,  1718, 11398,  1535,     4,\n",
      "          295,  9856,  2331,   579,  3432,   271,  7243,   399,    75,  2026,\n",
      "           14,   213, 26390,  7664,  1718,     6,   151, 11398,  1535,   669,\n",
      "        27353,     4,    31,     5,   569,     9,     5,  1901,    24,    16,\n",
      "          699,    14,    79,    26,  1718, 11398,  1535,    45,  1718,   151,\n",
      "        11398,  1535,     4,   128,   406, 10668,   428,   193,  1437,    22,\n",
      "          627,   168,    34,  7664,   733,  4963,   669, 27353,     8,   528,\n",
      "            7,    42,   476,  4033,    33,    57,  2906,     8, 47752,   365,\n",
      "            6,   151,  4963,    21,  5305,    60,     5,  2654,  1666,   108,\n",
      "          128,   267,   922,   195,     2,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1])\n"
     ]
    }
   ],
   "source": [
    "def tokenize_data(df, max_length = 256):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        df['claim'].tolist(),\n",
    "        df['top_k_docs'].fillna('').tolist(),  # Fill NaN with empty strings\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = df['label'].apply(lambda x: {'False': 0, 'True': 1, 'Conflicting': 2}[x]).tolist()\n",
    "    return tokenized_inputs, labels\n",
    "\n",
    "train_tokenized, train_labels = tokenize_data(train_df)\n",
    "val_tokenized, val_labels = tokenize_data(val_df)\n",
    "#test_tokenized, test_labels = tokenize_data(test_df)\n",
    "\n",
    "print(train_tokenized['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ClaimsDataset(Dataset):\n",
    "    def __init__(self, tokenized_inputs, labels):\n",
    "        self.input_ids = tokenized_inputs['input_ids']\n",
    "        self.attention_mask = tokenized_inputs['attention_mask']\n",
    "        self.labels = torch.tensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = ClaimsDataset(train_tokenized, train_labels)\n",
    "val_dataset = ClaimsDataset(val_tokenized, val_labels)\n",
    "#test_dataset = ClaimsDataset(test_tokenized, test_labels)\n",
    "\n",
    "train_tokenized = {key: val.to(device) for key, val in train_tokenized.items()}\n",
    "val_tokenized = {key: val.to(device) for key, val in val_tokenized.items()}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers= 4, pin_memory=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers= 4, pin_memory=False)\n",
    "#test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               claim  \\\n",
      "0  In her budget speech, Nirmala Sitharaman claim...   \n",
      "1  Florida residents affected by Hurricane Irma c...   \n",
      "2  Bill Gates' foundation tested a polio vaccine ...   \n",
      "\n",
      "                                          top_k_docs  \n",
      "0  conclusion: nirmala sitharaman didn't claim th...  \n",
      "1  'florida due to hurricane irma are greater tha...  \n",
      "2  '490,000 children paralyzed. \"bill gates found...  \n",
      "Batch 1\n",
      "{'input_ids': tensor([    0,  1121,    69,  1229,  1901,     6,   234,  9856,  2331, 33922,\n",
      "          271,  7243,  1695,    14,     5,  1621,  7664,  1718,     6,   151,\n",
      "         4963, 10918, 27353,    11,     5,   247,     4,     2,     2,  3865,\n",
      "        27953,    35,   295,  9856,  2331,   579,  3432,   271,  7243,   399,\n",
      "           75,  2026,    14,   213, 26390,  7664,  1718,     6,   151, 11398,\n",
      "         1535,   669, 27353,     4,    31,     5,   569,  9565,     5,  1901,\n",
      "           24,    16,   699,    14,    79,    26,  1718, 11398,  1535,     4,\n",
      "          295,  9856,  2331,   579,  3432,   271,  7243,   399,    75,  2026,\n",
      "           14,   213, 26390,  7664,  1718,     6,   151, 11398,  1535,   669,\n",
      "        27353,     4,    31,     5,   569,     9,     5,  1901,    24,    16,\n",
      "          699,    14,    79,    26,  1718, 11398,  1535,    45,  1718,   151,\n",
      "        11398,  1535,     4,   128,   406, 10668,   428,   193,  1437,    22,\n",
      "          627,   168,    34,  7664,   733,  4963,   669, 27353,     8,   528,\n",
      "            7,    42,   476,  4033,    33,    57,  2906,     8, 47752,   365,\n",
      "            6,   151,  4963,    21,  5305,    60,     5,  2654,  1666,   108,\n",
      "          128,   267,   922,   195,     2,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}\n",
      "Batch 2\n",
      "{'input_ids': tensor([    0, 31614,  1196,  2132,    30,  4370,  8547,    64,  1325,    68,\n",
      "        30586,    11,   689, 13236,  1795,     6,    53,   129,   114,    51,\n",
      "           64,   311,    14,    49,  1611,   685,   476,    13,    55,    87,\n",
      "           80,   722,     4,     2,     2,   108,  4825,   368,  4347,   528,\n",
      "            7,  6874,  1437,  9856,   102,    32,  2388,    87,  2004,  3485,\n",
      "         1932,   108,   128, 32643,   365,   193,  1437,  2077,  2342,   368,\n",
      "          808,  2071,    54,   685,   689,  1425,    50,  1038,   148,  6874,\n",
      "         1437,  9856,   102,   189,    28,  4973,    13,   780,  3591,   966,\n",
      "         2213,     9,  1932,     7,  1666,   108,   128, 14898, 33280,  1437,\n",
      "         9856,   102,  2322,  2342,   368,  4347,    15,   842,  3320, 22542,\n",
      "          158,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}\n",
      "Batch 3\n",
      "{'input_ids': tensor([    0, 19993, 10090,   108,  4811,  4776,    10, 30801,  9937,    11,\n",
      "          666,    14,   314,    23,   513, 40569,     6,   151,   408, 28582,\n",
      "            2,     2,   108, 30881,     6,   151,   408, 28582,     4,    22,\n",
      "        24916, 14213,  4811,  4776,    10, 30801,   748,  3631,  1666, 14213,\n",
      "         4811,    56,  4776, 30801,  9937,    11,  9473,   493,   227,  3788,\n",
      "          359,   193,     8,    56,  2242, 37596,   108,   128,  2028,   811,\n",
      "         3333,  4887,     5, 14213,   637,    13,    10,  7360,  9937,    12,\n",
      "         6031,  1851, 30801, 11004,    14, 28582,   204,  5607,     6,   151,\n",
      "          408,   227,  3788,     8,   193,     4,    11,   193,     5,  1666,\n",
      "          108,   128, 24837,    12,  2546,    12,   844,  1437,  9473,   811,\n",
      "         3333,  4887,     5, 14213,   637,    13,    10,  7360,  9937,    12,\n",
      "         6031,  1851, 30801, 11004,    14, 28582,   204,  5607,     6,   151,\n",
      "          408,   227,  3788,     8,   193,   955,   128,   179,  2428,     2,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(threshold=torch.inf)\n",
    "print(train_evidence_df[:3])\n",
    "\n",
    "for i, batch in enumerate(train_dataset):\n",
    "    assert batch['input_ids'].shape == batch['attention_mask'].shape\n",
    "    print(f\"Batch {i+1}\")\n",
    "    print(batch)\n",
    "    if i == 2:  # Only print the first 3 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory is set to: ./results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label weights: tensor([0.5740, 1.8138, 1.4151], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863de91939ea42d9908e79858f2e5b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2:\n",
      "Predictions: [1 1 1 0 1 1 1 0]\n",
      "Labels: [0 0 0 2 0 0 0 2]\n",
      "{'loss': 2.1111, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}\n",
      "Step 4:\n",
      "Predictions: [1 0 2 1 1 1 1 1]\n",
      "Labels: [0 1 1 0 0 0 2 2]\n",
      "{'loss': 1.6463, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}\n",
      "Step 6:\n",
      "Predictions: [1 2 0 2 1 0 1 1]\n",
      "Labels: [2 0 0 0 0 0 0 0]\n",
      "{'loss': 2.3153, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}\n",
      "Step 8:\n",
      "Predictions: [0 1 1 2 0 1 0 1]\n",
      "Labels: [1 0 1 0 0 0 0 1]\n",
      "{'loss': 1.5396, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.01}\n",
      "Step 10:\n",
      "Predictions: [1 0 1 1 1 1 1 1]\n",
      "Labels: [0 0 0 1 1 1 1 1]\n",
      "{'loss': 1.8784, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n",
      "Step 12:\n",
      "Predictions: [0 2 1 1 1 0 2 1]\n",
      "Labels: [1 0 1 0 0 2 0 0]\n",
      "{'loss': 1.3376, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.01}\n",
      "Step 14:\n",
      "Predictions: [1 0 1 2 1 1 1 1]\n",
      "Labels: [0 0 0 0 2 0 1 0]\n",
      "{'loss': 1.4406, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.01}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback, TrainerState, TrainerControl, AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "output_dir = './results'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "print(f\"Output directory is set to: {output_dir}\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    metrics = {}\n",
    "\n",
    "    # Compute macro F1-score\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    metrics['f1_macro'] = f1_macro\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    metrics['accuracy'] = accuracy_score(labels, preds)\n",
    "    \n",
    "    # Compute metrics for each label\n",
    "    for label_name, label_id in {'False': 0, 'True': 1, 'Conflicting': 2}.items():\n",
    "        precision, recall, f1_weighted, support = precision_recall_fscore_support(labels, preds, labels=[label_id], average='weighted')\n",
    "        metrics[f'{label_name}_precision'] = precision\n",
    "        metrics[f'{label_name}_recall'] = recall\n",
    "        metrics[f'{label_name}_f1_weighted'] = f1_weighted\n",
    "        metrics[f'{label_name}_support'] = support\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=2,\n",
    "    save_steps = 5000,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    )\n",
    "\n",
    "label_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=[0, 1, 2],\n",
    "    y=train_df['label'].apply(lambda x: {'False': 0, 'True': 1, 'Conflicting': 2}[x])\n",
    ")\n",
    "label_weights = torch.tensor(label_weights, dtype=torch.float).to(device)\n",
    "print(f\"Label weights: {label_weights}\")\n",
    "\n",
    "\n",
    "# logging predictions and labels to check if the model is learning\n",
    "class PredictionLoggerCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % args.logging_steps == 0:\n",
    "            model = kwargs.get('model')\n",
    "            dataloader = kwargs.get('train_dataloader')\n",
    "            \n",
    "            if model and dataloader:\n",
    "                model.eval()\n",
    "                batch = next(iter(dataloader))\n",
    "                inputs = {key: value.to(args.device) for key, value in batch.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    preds = torch.argmax(logits, axis=-1)\n",
    "                    labels = inputs['labels']\n",
    "                    \n",
    "                    print(f\"Step {state.global_step}:\")\n",
    "                    print(f\"Predictions: {preds.cpu().numpy()}\")\n",
    "                    print(f\"Labels: {labels.cpu().numpy()}\")\n",
    "                model.train()\n",
    "\n",
    "        return control\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=label_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "log_callback = PredictionLoggerCallback()\n",
    "\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(optimizer, None),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[log_callback]\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save eval results to file\n",
    "results = trainer.evaluate(val_dataset)\n",
    "\n",
    "results_file = os.path.join(training_args.output_dir, \"evaluation_results.txt\")\n",
    "with open(results_file, \"w\") as writer:\n",
    "    for key, value in results.items():\n",
    "        writer.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Run predictions on the test set and identify failed cases\n",
    "predictions_output = trainer.predict(val_dataset)\n",
    "predictions = np.argmax(predictions_output.predictions, axis=1)\n",
    "labels = predictions_output.label_ids\n",
    "failed_indices = np.where(predictions != labels)[0]\n",
    "\n",
    "# Get claims and predictions\n",
    "failed_claims = val_df.iloc[failed_indices]\n",
    "failed_predictions = predictions[failed_indices]\n",
    "failed_labels = labels[failed_indices]\n",
    "\n",
    "# Randomly select claims for qualitative analysis\n",
    "num_samples = 20\n",
    "random_indices = random.sample(range(len(failed_indices)), min(num_samples, len(failed_indices)))\n",
    "\n",
    "# Extract the randomly selected failed claims, predictions, and the labels they should have had\n",
    "selected_failed_claims = failed_claims.iloc[random_indices]\n",
    "selected_failed_predictions = failed_predictions[random_indices]\n",
    "selected_failed_labels = failed_labels[random_indices]\n",
    "\n",
    "# DataFrame for qualitative analysis\n",
    "qualitative_analysis_df = pd.DataFrame({\n",
    "    'claim': selected_failed_claims['claim'].values,\n",
    "    'top_k_docs': selected_failed_claims['top_k_docs'].values,\n",
    "    'real_label': selected_failed_labels,\n",
    "    'predicted_label': selected_failed_predictions\n",
    "})\n",
    "\n",
    "# Mapping numerical labels to their string representations\n",
    "label_map = {0: 'False', 1: 'True', 2: 'Conflicting'}\n",
    "qualitative_analysis_df['real_label'] = qualitative_analysis_df['real_label'].map(label_map)\n",
    "qualitative_analysis_df['predicted_label'] = qualitative_analysis_df['predicted_label'].map(label_map)\n",
    "\n",
    "# Saving the results to a text file\n",
    "qualitative_analysis_file = os.path.join(training_args.output_dir, \"qualitative_analysis.txt\")\n",
    "with open(qualitative_analysis_file, \"w\") as writer:\n",
    "    for index, row in qualitative_analysis_df.iterrows():\n",
    "        writer.write(f\"Claim: {row['claim']}\\n\")\n",
    "        writer.write(f\"Evidence: {row['top_k_docs']}\\n\")\n",
    "        writer.write(f\"Real Label: {row['real_label']}\\n\")\n",
    "        writer.write(f\"Predicted Label: {row['predicted_label']}\\n\")\n",
    "        writer.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just checking if the model is learning\n",
    "train_label_counts = pd.Series(train_labels).value_counts()\n",
    "print(train_label_counts)\n",
    "val_label_counts = pd.Series(val_labels).value_counts()\n",
    "print(val_label_counts)\n",
    "prediction_label_counts = pd.Series(predictions).value_counts()\n",
    "print(prediction_label_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
