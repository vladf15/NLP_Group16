{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\", num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#inspired by the oracle code\n",
    "#https://colab.research.google.com/drive/1gZJCakmY28cKGMj8B7wd1GUM3r72pdbi?usp=sharing#scrollTo=8eKFjiC3i8Yx\n",
    "# Function to get labels and claims from the dataset\n",
    "\n",
    "train_df = pd.read_json('train_claims_quantemp.json')\n",
    "val_df = pd.read_json('val_claims_quantemp.json')\n",
    "test_df = pd.read_json('test_claims_quantemp.json')\n",
    "\n",
    "train_df = pd.DataFrame([{'label': item['label'], 'claim': item['claim']} for item in train_df.to_dict(orient='records')])\n",
    "val_df = pd.DataFrame([{'label': item['label'], 'claim': item['claim']} for item in val_df.to_dict(orient='records')])\n",
    "test_df = pd.DataFrame([{'label': item['label'], 'claim': item['claim']} for item in test_df.to_dict(orient='records')])\n",
    "\n",
    "#old evidence file for bm25\n",
    "#evidence_df = pd.read_json('bm25_top_100_claimdecomp.json')\n",
    "#evidence_df = pd.DataFrame([{'claim': item['claim'], 'docs': item['docs'], 'scores': item['scores']} for item in evidence_df.to_dict(orient='records')])\n",
    "\n",
    "#load csv evidence files for training and validation\n",
    "train_evidence_df = pd.read_csv('NLP_Group16/evidences_train.csv')\n",
    "val_evidence_df = pd.read_csv('NLP_Group16/evidences_val.csv')\n",
    "train_evidence_df = train_evidence_df[['claim', 'evidences', 'scores']]\n",
    "val_evidence_df = val_evidence_df[['claim', 'evidences', 'scores']]\n",
    "\n",
    "\n",
    "# Get top_k relevant evidence for each claim\n",
    "def combine_top_k_evidence(row, top_k):\n",
    "    evidences = row['evidences'].strip('[]').split(', ')\n",
    "    scores = list(map(float, row['scores'].strip('[]').split(', ')))\n",
    "    ranked_documents = sorted(zip(evidences, scores), key=lambda x: x[1], reverse=True)\n",
    "    top_k_documents = [doc.strip('\"') for doc, score in ranked_documents[:top_k]]\n",
    "    return ' '.join(top_k_documents)\n",
    "\n",
    "top_k = 5\n",
    "## Apply the function to get top_k evidence for each claim\n",
    "train_evidence_df['top_k_docs'] = train_evidence_df.apply(lambda row: combine_top_k_evidence(row, top_k), axis=1)\n",
    "val_evidence_df['top_k_docs'] = val_evidence_df.apply(lambda row: combine_top_k_evidence(row, top_k), axis=1)\n",
    "\n",
    "# Drop docs and scores columns as they are no longer needed\n",
    "train_evidence_df = train_evidence_df.drop(columns=['evidences', 'scores'])\n",
    "val_evidence_df = val_evidence_df.drop(columns=['evidences', 'scores'])\n",
    "\n",
    "# Merge the combined evidence with the train, validation, and test DataFrames\n",
    "train_df = train_df.merge(train_evidence_df[['claim', 'top_k_docs']], on='claim', how='left')\n",
    "val_df = val_df.merge(val_evidence_df[['claim', 'top_k_docs']], on='claim', how='left')\n",
    "#test_df = test_df.merge(evidence_df[['claim', 'top_k_docs']], on='claim', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'top_k_docs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'top_k_docs'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m train_tokenized, train_labels \u001b[38;5;241m=\u001b[39m tokenize_data(train_df)\n\u001b[0;32m     14\u001b[0m val_tokenized, val_labels \u001b[38;5;241m=\u001b[39m tokenize_data(val_df)\n\u001b[1;32m---> 15\u001b[0m test_tokenized, test_labels \u001b[38;5;241m=\u001b[39m tokenize_data(test_df)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_tokenized[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m, in \u001b[0;36mtokenize_data\u001b[1;34m(df, max_length)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_data\u001b[39m(df, max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m):\n\u001b[0;32m      2\u001b[0m     tokenized_inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m      3\u001b[0m         df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclaim\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m----> 4\u001b[0m         df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_k_docs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist(),  \u001b[38;5;66;03m# Fill NaN with empty strings\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m      8\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m     )\n\u001b[0;32m     10\u001b[0m     labels \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFalse\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConflicting\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m}[x])\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_inputs, labels\n",
      "File \u001b[1;32mc:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'top_k_docs'"
     ]
    }
   ],
   "source": [
    "def tokenize_data(df, max_length = 256):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        df['claim'].tolist(),\n",
    "        df['top_k_docs'].fillna('').tolist(),  # Fill NaN with empty strings\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = df['label'].apply(lambda x: {'True': 0, 'False': 1, 'Conflicting': 2}[x]).tolist()\n",
    "    return tokenized_inputs, labels\n",
    "\n",
    "train_tokenized, train_labels = tokenize_data(train_df)\n",
    "val_tokenized, val_labels = tokenize_data(val_df)\n",
    "test_tokenized, test_labels = tokenize_data(test_df)\n",
    "\n",
    "print(train_tokenized['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ClaimsDataset(Dataset):\n",
    "    def __init__(self, tokenized_inputs, labels):\n",
    "        self.input_ids = tokenized_inputs['input_ids']\n",
    "        self.attention_mask = tokenized_inputs['attention_mask']\n",
    "        self.labels = torch.tensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = ClaimsDataset(train_tokenized, train_labels)\n",
    "val_dataset = ClaimsDataset(val_tokenized, val_labels)\n",
    "#test_dataset = ClaimsDataset(test_tokenized, test_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, pin_memory=True)\n",
    "#test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory is set to: ./results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd8bd85f3aa4eb2bc567e9f7f3a40b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2931, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 1.1886, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}\n",
      "{'loss': 0.9793, 'learning_rate': 5.2e-06, 'epoch': 0.01}\n",
      "{'loss': 1.2977, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.02}\n",
      "{'loss': 1.0203, 'learning_rate': 9e-06, 'epoch': 0.02}\n",
      "{'loss': 1.0908, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 0.9977, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.03}\n",
      "{'loss': 0.9946, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.03}\n",
      "{'loss': 1.2494, 'learning_rate': 1.7e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9635, 'learning_rate': 1.9e-05, 'epoch': 0.04}\n",
      "{'loss': 0.8767, 'learning_rate': 1.995805369127517e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9054, 'learning_rate': 1.9874161073825505e-05, 'epoch': 0.05}\n",
      "{'loss': 0.9843, 'learning_rate': 1.979026845637584e-05, 'epoch': 0.05}\n",
      "{'loss': 1.109, 'learning_rate': 1.9706375838926174e-05, 'epoch': 0.06}\n",
      "{'loss': 0.9737, 'learning_rate': 1.962248322147651e-05, 'epoch': 0.06}\n",
      "{'loss': 0.8591, 'learning_rate': 1.9546979865771813e-05, 'epoch': 0.06}\n",
      "{'loss': 1.0116, 'learning_rate': 1.946308724832215e-05, 'epoch': 0.07}\n",
      "{'loss': 0.9772, 'learning_rate': 1.9379194630872483e-05, 'epoch': 0.07}\n",
      "{'loss': 0.9962, 'learning_rate': 1.929530201342282e-05, 'epoch': 0.08}\n",
      "{'loss': 1.0216, 'learning_rate': 1.9211409395973156e-05, 'epoch': 0.08}\n",
      "{'loss': 0.8881, 'learning_rate': 1.9127516778523493e-05, 'epoch': 0.08}\n",
      "{'loss': 0.9878, 'learning_rate': 1.904362416107383e-05, 'epoch': 0.09}\n",
      "{'loss': 0.996, 'learning_rate': 1.8959731543624162e-05, 'epoch': 0.09}\n",
      "{'loss': 1.074, 'learning_rate': 1.88758389261745e-05, 'epoch': 0.1}\n",
      "{'loss': 0.8136, 'learning_rate': 1.8791946308724832e-05, 'epoch': 0.1}\n",
      "{'loss': 0.7699, 'learning_rate': 1.870805369127517e-05, 'epoch': 0.1}\n",
      "{'loss': 0.8755, 'learning_rate': 1.8624161073825505e-05, 'epoch': 0.11}\n",
      "{'loss': 0.7939, 'learning_rate': 1.854026845637584e-05, 'epoch': 0.11}\n",
      "{'loss': 1.0365, 'learning_rate': 1.8456375838926174e-05, 'epoch': 0.12}\n",
      "{'loss': 0.7932, 'learning_rate': 1.837248322147651e-05, 'epoch': 0.12}\n",
      "{'loss': 0.9522, 'learning_rate': 1.8288590604026847e-05, 'epoch': 0.12}\n",
      "{'loss': 0.8541, 'learning_rate': 1.820469798657718e-05, 'epoch': 0.13}\n",
      "{'loss': 0.9485, 'learning_rate': 1.8120805369127517e-05, 'epoch': 0.13}\n",
      "{'loss': 1.0605, 'learning_rate': 1.8036912751677854e-05, 'epoch': 0.14}\n",
      "{'loss': 0.8966, 'learning_rate': 1.795302013422819e-05, 'epoch': 0.14}\n",
      "{'loss': 0.8738, 'learning_rate': 1.7869127516778523e-05, 'epoch': 0.14}\n",
      "{'loss': 0.5055, 'learning_rate': 1.778523489932886e-05, 'epoch': 0.15}\n",
      "{'loss': 0.5645, 'learning_rate': 1.7701342281879196e-05, 'epoch': 0.15}\n",
      "{'loss': 1.1451, 'learning_rate': 1.7617449664429533e-05, 'epoch': 0.16}\n",
      "{'loss': 0.9392, 'learning_rate': 1.753355704697987e-05, 'epoch': 0.16}\n",
      "{'loss': 1.0765, 'learning_rate': 1.7449664429530202e-05, 'epoch': 0.17}\n",
      "{'loss': 1.0521, 'learning_rate': 1.736577181208054e-05, 'epoch': 0.17}\n",
      "{'loss': 0.8164, 'learning_rate': 1.7281879194630872e-05, 'epoch': 0.17}\n",
      "{'loss': 0.943, 'learning_rate': 1.719798657718121e-05, 'epoch': 0.18}\n",
      "{'loss': 1.0072, 'learning_rate': 1.7114093959731545e-05, 'epoch': 0.18}\n",
      "{'loss': 0.9143, 'learning_rate': 1.703020134228188e-05, 'epoch': 0.19}\n",
      "{'loss': 0.9464, 'learning_rate': 1.6946308724832218e-05, 'epoch': 0.19}\n",
      "{'loss': 0.7812, 'learning_rate': 1.686241610738255e-05, 'epoch': 0.19}\n",
      "{'loss': 0.9506, 'learning_rate': 1.6778523489932888e-05, 'epoch': 0.2}\n",
      "{'loss': 0.7854, 'learning_rate': 1.669463087248322e-05, 'epoch': 0.2}\n",
      "{'loss': 0.6717, 'learning_rate': 1.6610738255033557e-05, 'epoch': 0.21}\n",
      "{'loss': 0.9459, 'learning_rate': 1.6526845637583894e-05, 'epoch': 0.21}\n",
      "{'loss': 0.6912, 'learning_rate': 1.644295302013423e-05, 'epoch': 0.21}\n",
      "{'loss': 0.995, 'learning_rate': 1.6359060402684567e-05, 'epoch': 0.22}\n",
      "{'loss': 0.7301, 'learning_rate': 1.62751677852349e-05, 'epoch': 0.22}\n",
      "{'loss': 0.995, 'learning_rate': 1.6191275167785237e-05, 'epoch': 0.23}\n",
      "{'loss': 1.0459, 'learning_rate': 1.610738255033557e-05, 'epoch': 0.23}\n",
      "{'loss': 0.9154, 'learning_rate': 1.6023489932885906e-05, 'epoch': 0.23}\n",
      "{'loss': 0.796, 'learning_rate': 1.5939597315436243e-05, 'epoch': 0.24}\n",
      "{'loss': 0.8955, 'learning_rate': 1.585570469798658e-05, 'epoch': 0.24}\n",
      "{'loss': 0.9437, 'learning_rate': 1.5771812080536916e-05, 'epoch': 0.25}\n",
      "{'loss': 0.7663, 'learning_rate': 1.568791946308725e-05, 'epoch': 0.25}\n",
      "{'loss': 1.097, 'learning_rate': 1.5604026845637585e-05, 'epoch': 0.25}\n",
      "{'loss': 0.9303, 'learning_rate': 1.552013422818792e-05, 'epoch': 0.26}\n",
      "{'loss': 0.9319, 'learning_rate': 1.5436241610738255e-05, 'epoch': 0.26}\n",
      "{'loss': 0.7044, 'learning_rate': 1.535234899328859e-05, 'epoch': 0.27}\n",
      "{'loss': 0.9286, 'learning_rate': 1.5268456375838928e-05, 'epoch': 0.27}\n",
      "{'loss': 0.9469, 'learning_rate': 1.5184563758389263e-05, 'epoch': 0.27}\n",
      "{'loss': 0.8788, 'learning_rate': 1.5100671140939598e-05, 'epoch': 0.28}\n",
      "{'loss': 0.6458, 'learning_rate': 1.5016778523489934e-05, 'epoch': 0.28}\n",
      "{'loss': 1.2372, 'learning_rate': 1.4932885906040269e-05, 'epoch': 0.29}\n",
      "{'loss': 0.7939, 'learning_rate': 1.4848993288590605e-05, 'epoch': 0.29}\n",
      "{'loss': 1.1275, 'learning_rate': 1.4765100671140942e-05, 'epoch': 0.29}\n",
      "{'loss': 0.9038, 'learning_rate': 1.4681208053691277e-05, 'epoch': 0.3}\n",
      "{'loss': 1.1047, 'learning_rate': 1.4597315436241613e-05, 'epoch': 0.3}\n",
      "{'loss': 0.9419, 'learning_rate': 1.4513422818791946e-05, 'epoch': 0.31}\n",
      "{'loss': 0.9539, 'learning_rate': 1.4429530201342283e-05, 'epoch': 0.31}\n",
      "{'loss': 0.8487, 'learning_rate': 1.4345637583892618e-05, 'epoch': 0.31}\n",
      "{'loss': 0.8185, 'learning_rate': 1.4261744966442954e-05, 'epoch': 0.32}\n",
      "{'loss': 0.8618, 'learning_rate': 1.417785234899329e-05, 'epoch': 0.32}\n",
      "{'loss': 0.7808, 'learning_rate': 1.4093959731543626e-05, 'epoch': 0.33}\n",
      "{'loss': 0.9278, 'learning_rate': 1.4010067114093962e-05, 'epoch': 0.33}\n",
      "{'loss': 1.0101, 'learning_rate': 1.3926174496644295e-05, 'epoch': 0.33}\n",
      "{'loss': 1.0259, 'learning_rate': 1.3842281879194632e-05, 'epoch': 0.34}\n",
      "{'loss': 0.859, 'learning_rate': 1.3758389261744966e-05, 'epoch': 0.34}\n",
      "{'loss': 1.0023, 'learning_rate': 1.3674496644295303e-05, 'epoch': 0.35}\n",
      "{'loss': 0.8213, 'learning_rate': 1.359060402684564e-05, 'epoch': 0.35}\n",
      "{'loss': 0.9276, 'learning_rate': 1.3506711409395974e-05, 'epoch': 0.35}\n",
      "{'loss': 0.9039, 'learning_rate': 1.342281879194631e-05, 'epoch': 0.36}\n",
      "{'loss': 0.827, 'learning_rate': 1.3338926174496644e-05, 'epoch': 0.36}\n",
      "{'loss': 0.9749, 'learning_rate': 1.325503355704698e-05, 'epoch': 0.37}\n",
      "{'loss': 1.2273, 'learning_rate': 1.3179530201342283e-05, 'epoch': 0.37}\n",
      "{'loss': 0.7547, 'learning_rate': 1.3095637583892618e-05, 'epoch': 0.37}\n",
      "{'loss': 0.6833, 'learning_rate': 1.3011744966442954e-05, 'epoch': 0.38}\n",
      "{'loss': 0.9386, 'learning_rate': 1.292785234899329e-05, 'epoch': 0.38}\n",
      "{'loss': 0.8875, 'learning_rate': 1.2843959731543626e-05, 'epoch': 0.39}\n",
      "{'loss': 0.8683, 'learning_rate': 1.2760067114093962e-05, 'epoch': 0.39}\n",
      "{'loss': 0.8412, 'learning_rate': 1.2676174496644295e-05, 'epoch': 0.39}\n",
      "{'loss': 1.01, 'learning_rate': 1.2592281879194632e-05, 'epoch': 0.4}\n",
      "{'loss': 0.7212, 'learning_rate': 1.2508389261744967e-05, 'epoch': 0.4}\n",
      "{'loss': 0.9291, 'learning_rate': 1.2424496644295303e-05, 'epoch': 0.41}\n",
      "{'loss': 0.7253, 'learning_rate': 1.234060402684564e-05, 'epoch': 0.41}\n",
      "{'loss': 0.6172, 'learning_rate': 1.2256711409395974e-05, 'epoch': 0.41}\n",
      "{'loss': 1.0073, 'learning_rate': 1.2172818791946311e-05, 'epoch': 0.42}\n",
      "{'loss': 0.9329, 'learning_rate': 1.2088926174496644e-05, 'epoch': 0.42}\n",
      "{'loss': 1.0176, 'learning_rate': 1.200503355704698e-05, 'epoch': 0.43}\n",
      "{'loss': 0.9129, 'learning_rate': 1.1921140939597315e-05, 'epoch': 0.43}\n",
      "{'loss': 0.788, 'learning_rate': 1.1837248322147652e-05, 'epoch': 0.43}\n",
      "{'loss': 0.885, 'learning_rate': 1.1753355704697988e-05, 'epoch': 0.44}\n",
      "{'loss': 0.9716, 'learning_rate': 1.1669463087248323e-05, 'epoch': 0.44}\n",
      "{'loss': 1.0572, 'learning_rate': 1.158557046979866e-05, 'epoch': 0.45}\n",
      "{'loss': 0.9325, 'learning_rate': 1.1501677852348993e-05, 'epoch': 0.45}\n",
      "{'loss': 0.9358, 'learning_rate': 1.141778523489933e-05, 'epoch': 0.45}\n",
      "{'loss': 0.8922, 'learning_rate': 1.1333892617449664e-05, 'epoch': 0.46}\n",
      "{'loss': 1.1516, 'learning_rate': 1.125e-05, 'epoch': 0.46}\n",
      "{'loss': 0.7012, 'learning_rate': 1.1166107382550337e-05, 'epoch': 0.47}\n",
      "{'loss': 0.8897, 'learning_rate': 1.1082214765100672e-05, 'epoch': 0.47}\n",
      "{'loss': 0.6848, 'learning_rate': 1.0998322147651009e-05, 'epoch': 0.48}\n",
      "{'loss': 0.9047, 'learning_rate': 1.0914429530201343e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1541, 'learning_rate': 1.083053691275168e-05, 'epoch': 0.48}\n",
      "{'loss': 0.8829, 'learning_rate': 1.0746644295302013e-05, 'epoch': 0.49}\n",
      "{'loss': 0.8228, 'learning_rate': 1.066275167785235e-05, 'epoch': 0.49}\n",
      "{'loss': 0.8737, 'learning_rate': 1.0578859060402686e-05, 'epoch': 0.5}\n",
      "{'loss': 0.8169, 'learning_rate': 1.049496644295302e-05, 'epoch': 0.5}\n",
      "{'loss': 0.8569, 'learning_rate': 1.0411073825503357e-05, 'epoch': 0.5}\n",
      "{'loss': 0.8491, 'learning_rate': 1.0327181208053692e-05, 'epoch': 0.51}\n",
      "{'loss': 0.7502, 'learning_rate': 1.0243288590604029e-05, 'epoch': 0.51}\n",
      "{'loss': 0.8017, 'learning_rate': 1.0159395973154362e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7812, 'learning_rate': 1.0075503355704698e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7212, 'learning_rate': 9.991610738255035e-06, 'epoch': 0.52}\n",
      "{'loss': 0.7377, 'learning_rate': 9.90771812080537e-06, 'epoch': 0.53}\n",
      "{'loss': 0.6717, 'learning_rate': 9.823825503355704e-06, 'epoch': 0.53}\n",
      "{'loss': 0.7114, 'learning_rate': 9.739932885906041e-06, 'epoch': 0.54}\n",
      "{'loss': 1.1376, 'learning_rate': 9.656040268456377e-06, 'epoch': 0.54}\n",
      "{'loss': 0.9749, 'learning_rate': 9.572147651006712e-06, 'epoch': 0.54}\n",
      "{'loss': 0.7762, 'learning_rate': 9.488255033557047e-06, 'epoch': 0.55}\n",
      "{'loss': 1.0774, 'learning_rate': 9.404362416107384e-06, 'epoch': 0.55}\n",
      "{'loss': 0.7319, 'learning_rate': 9.320469798657718e-06, 'epoch': 0.56}\n",
      "{'loss': 0.7117, 'learning_rate': 9.236577181208053e-06, 'epoch': 0.56}\n",
      "{'loss': 0.6746, 'learning_rate': 9.15268456375839e-06, 'epoch': 0.56}\n",
      "{'loss': 0.8987, 'learning_rate': 9.068791946308726e-06, 'epoch': 0.57}\n",
      "{'loss': 0.842, 'learning_rate': 8.984899328859061e-06, 'epoch': 0.57}\n",
      "{'loss': 0.9346, 'learning_rate': 8.901006711409398e-06, 'epoch': 0.58}\n",
      "{'loss': 0.7652, 'learning_rate': 8.817114093959732e-06, 'epoch': 0.58}\n",
      "{'loss': 0.9491, 'learning_rate': 8.733221476510067e-06, 'epoch': 0.58}\n",
      "{'loss': 1.0523, 'learning_rate': 8.649328859060404e-06, 'epoch': 0.59}\n",
      "{'loss': 1.008, 'learning_rate': 8.56543624161074e-06, 'epoch': 0.59}\n",
      "{'loss': 0.9888, 'learning_rate': 8.481543624161075e-06, 'epoch': 0.6}\n",
      "{'loss': 0.9914, 'learning_rate': 8.39765100671141e-06, 'epoch': 0.6}\n",
      "{'loss': 0.7146, 'learning_rate': 8.313758389261746e-06, 'epoch': 0.6}\n",
      "{'loss': 0.9248, 'learning_rate': 8.229865771812081e-06, 'epoch': 0.61}\n",
      "{'loss': 0.8422, 'learning_rate': 8.145973154362416e-06, 'epoch': 0.61}\n",
      "{'loss': 0.7916, 'learning_rate': 8.062080536912753e-06, 'epoch': 0.62}\n",
      "{'loss': 1.0165, 'learning_rate': 7.978187919463087e-06, 'epoch': 0.62}\n",
      "{'loss': 0.8115, 'learning_rate': 7.894295302013424e-06, 'epoch': 0.62}\n",
      "{'loss': 0.8449, 'learning_rate': 7.810402684563759e-06, 'epoch': 0.63}\n",
      "{'loss': 0.6763, 'learning_rate': 7.726510067114095e-06, 'epoch': 0.63}\n",
      "{'loss': 0.7962, 'learning_rate': 7.64261744966443e-06, 'epoch': 0.64}\n",
      "{'loss': 0.8741, 'learning_rate': 7.558724832214766e-06, 'epoch': 0.64}\n",
      "{'loss': 0.7617, 'learning_rate': 7.474832214765101e-06, 'epoch': 0.64}\n",
      "{'loss': 0.688, 'learning_rate': 7.390939597315436e-06, 'epoch': 0.65}\n",
      "{'loss': 0.9058, 'learning_rate': 7.307046979865773e-06, 'epoch': 0.65}\n",
      "{'loss': 1.0059, 'learning_rate': 7.223154362416108e-06, 'epoch': 0.66}\n",
      "{'loss': 0.7541, 'learning_rate': 7.139261744966444e-06, 'epoch': 0.66}\n",
      "{'loss': 0.7324, 'learning_rate': 7.055369127516779e-06, 'epoch': 0.66}\n",
      "{'loss': 1.0927, 'learning_rate': 6.9714765100671144e-06, 'epoch': 0.67}\n",
      "{'loss': 0.8513, 'learning_rate': 6.88758389261745e-06, 'epoch': 0.67}\n",
      "{'loss': 0.5815, 'learning_rate': 6.803691275167785e-06, 'epoch': 0.68}\n",
      "{'loss': 0.9631, 'learning_rate': 6.7197986577181214e-06, 'epoch': 0.68}\n",
      "{'loss': 0.8521, 'learning_rate': 6.635906040268457e-06, 'epoch': 0.68}\n",
      "{'loss': 0.7831, 'learning_rate': 6.552013422818793e-06, 'epoch': 0.69}\n",
      "{'loss': 0.8605, 'learning_rate': 6.468120805369128e-06, 'epoch': 0.69}\n",
      "{'loss': 0.8552, 'learning_rate': 6.384228187919463e-06, 'epoch': 0.7}\n",
      "{'loss': 0.6852, 'learning_rate': 6.300335570469799e-06, 'epoch': 0.7}\n",
      "{'loss': 1.0139, 'learning_rate': 6.2248322147651006e-06, 'epoch': 0.7}\n",
      "{'loss': 0.7277, 'learning_rate': 6.140939597315436e-06, 'epoch': 0.71}\n",
      "{'loss': 0.9729, 'learning_rate': 6.057046979865773e-06, 'epoch': 0.71}\n",
      "{'loss': 0.7551, 'learning_rate': 5.973154362416108e-06, 'epoch': 0.72}\n",
      "{'loss': 0.7778, 'learning_rate': 5.889261744966443e-06, 'epoch': 0.72}\n",
      "{'loss': 0.7736, 'learning_rate': 5.805369127516779e-06, 'epoch': 0.72}\n",
      "{'loss': 0.6873, 'learning_rate': 5.7214765100671146e-06, 'epoch': 0.73}\n",
      "{'loss': 1.0494, 'learning_rate': 5.637583892617449e-06, 'epoch': 0.73}\n",
      "{'loss': 0.982, 'learning_rate': 5.553691275167785e-06, 'epoch': 0.74}\n",
      "{'loss': 0.8559, 'learning_rate': 5.4697986577181215e-06, 'epoch': 0.74}\n",
      "{'loss': 0.6909, 'learning_rate': 5.385906040268457e-06, 'epoch': 0.74}\n",
      "{'loss': 0.9495, 'learning_rate': 5.302013422818793e-06, 'epoch': 0.75}\n",
      "{'loss': 0.735, 'learning_rate': 5.218120805369128e-06, 'epoch': 0.75}\n",
      "{'loss': 0.9096, 'learning_rate': 5.134228187919463e-06, 'epoch': 0.76}\n",
      "{'loss': 0.7682, 'learning_rate': 5.050335570469799e-06, 'epoch': 0.76}\n",
      "{'loss': 0.901, 'learning_rate': 4.966442953020135e-06, 'epoch': 0.76}\n",
      "{'loss': 0.8622, 'learning_rate': 4.88255033557047e-06, 'epoch': 0.77}\n",
      "{'loss': 0.8417, 'learning_rate': 4.798657718120805e-06, 'epoch': 0.77}\n",
      "{'loss': 0.7122, 'learning_rate': 4.714765100671142e-06, 'epoch': 0.78}\n",
      "{'loss': 0.6432, 'learning_rate': 4.6308724832214765e-06, 'epoch': 0.78}\n",
      "{'loss': 0.7151, 'learning_rate': 4.546979865771812e-06, 'epoch': 0.79}\n",
      "{'loss': 1.1174, 'learning_rate': 4.463087248322149e-06, 'epoch': 0.79}\n",
      "{'loss': 0.9207, 'learning_rate': 4.3791946308724835e-06, 'epoch': 0.79}\n",
      "{'loss': 0.5986, 'learning_rate': 4.295302013422819e-06, 'epoch': 0.8}\n",
      "{'loss': 0.8226, 'learning_rate': 4.211409395973155e-06, 'epoch': 0.8}\n",
      "{'loss': 0.9045, 'learning_rate': 4.1275167785234905e-06, 'epoch': 0.81}\n",
      "{'loss': 0.5975, 'learning_rate': 4.043624161073826e-06, 'epoch': 0.81}\n",
      "{'loss': 0.8672, 'learning_rate': 3.959731543624161e-06, 'epoch': 0.81}\n",
      "{'loss': 0.9706, 'learning_rate': 3.8758389261744974e-06, 'epoch': 0.82}\n",
      "{'loss': 0.7432, 'learning_rate': 3.7919463087248327e-06, 'epoch': 0.82}\n",
      "{'loss': 0.8403, 'learning_rate': 3.708053691275168e-06, 'epoch': 0.83}\n",
      "{'loss': 0.6744, 'learning_rate': 3.6241610738255036e-06, 'epoch': 0.83}\n",
      "{'loss': 0.7413, 'learning_rate': 3.5402684563758393e-06, 'epoch': 0.83}\n",
      "{'loss': 0.9299, 'learning_rate': 3.456375838926175e-06, 'epoch': 0.84}\n",
      "{'loss': 1.1132, 'learning_rate': 3.37248322147651e-06, 'epoch': 0.84}\n",
      "{'loss': 0.9219, 'learning_rate': 3.2885906040268462e-06, 'epoch': 0.85}\n",
      "{'loss': 1.1215, 'learning_rate': 3.2046979865771815e-06, 'epoch': 0.85}\n",
      "{'loss': 0.6867, 'learning_rate': 3.120805369127517e-06, 'epoch': 0.85}\n",
      "{'loss': 0.7303, 'learning_rate': 3.0369127516778524e-06, 'epoch': 0.86}\n",
      "{'loss': 0.9269, 'learning_rate': 2.9530201342281885e-06, 'epoch': 0.86}\n",
      "{'loss': 0.7437, 'learning_rate': 2.8691275167785237e-06, 'epoch': 0.87}\n",
      "{'loss': 0.7318, 'learning_rate': 2.785234899328859e-06, 'epoch': 0.87}\n",
      "{'loss': 0.7937, 'learning_rate': 2.701342281879195e-06, 'epoch': 0.87}\n",
      "{'loss': 0.6803, 'learning_rate': 2.6174496644295307e-06, 'epoch': 0.88}\n",
      "{'loss': 0.8191, 'learning_rate': 2.533557046979866e-06, 'epoch': 0.88}\n",
      "{'loss': 1.0154, 'learning_rate': 2.4496644295302016e-06, 'epoch': 0.89}\n",
      "{'loss': 0.8078, 'learning_rate': 2.365771812080537e-06, 'epoch': 0.89}\n",
      "{'loss': 0.7991, 'learning_rate': 2.2818791946308725e-06, 'epoch': 0.89}\n",
      "{'loss': 0.8519, 'learning_rate': 2.197986577181208e-06, 'epoch': 0.9}\n",
      "{'loss': 0.9521, 'learning_rate': 2.114093959731544e-06, 'epoch': 0.9}\n",
      "{'loss': 0.6907, 'learning_rate': 2.0302013422818795e-06, 'epoch': 0.91}\n",
      "{'loss': 0.6535, 'learning_rate': 1.9463087248322147e-06, 'epoch': 0.91}\n",
      "{'loss': 0.8412, 'learning_rate': 1.8624161073825506e-06, 'epoch': 0.91}\n",
      "{'loss': 0.6852, 'learning_rate': 1.778523489932886e-06, 'epoch': 0.92}\n",
      "{'loss': 0.9278, 'learning_rate': 1.6946308724832217e-06, 'epoch': 0.92}\n",
      "{'loss': 0.8148, 'learning_rate': 1.6107382550335572e-06, 'epoch': 0.93}\n",
      "{'loss': 0.5851, 'learning_rate': 1.5268456375838928e-06, 'epoch': 0.93}\n",
      "{'loss': 0.9991, 'learning_rate': 1.4429530201342285e-06, 'epoch': 0.93}\n",
      "{'loss': 0.9025, 'learning_rate': 1.359060402684564e-06, 'epoch': 0.94}\n",
      "{'loss': 0.8501, 'learning_rate': 1.2751677852348996e-06, 'epoch': 0.94}\n",
      "{'loss': 0.7387, 'learning_rate': 1.1912751677852349e-06, 'epoch': 0.95}\n",
      "{'loss': 0.7598, 'learning_rate': 1.1073825503355707e-06, 'epoch': 0.95}\n",
      "{'loss': 0.8472, 'learning_rate': 1.0234899328859062e-06, 'epoch': 0.95}\n",
      "{'loss': 0.4893, 'learning_rate': 9.395973154362417e-07, 'epoch': 0.96}\n",
      "{'loss': 0.7277, 'learning_rate': 8.557046979865773e-07, 'epoch': 0.96}\n",
      "{'loss': 0.6443, 'learning_rate': 7.718120805369129e-07, 'epoch': 0.97}\n",
      "{'loss': 0.8732, 'learning_rate': 6.879194630872484e-07, 'epoch': 0.97}\n",
      "{'loss': 0.8713, 'learning_rate': 6.04026845637584e-07, 'epoch': 0.97}\n",
      "{'loss': 0.7656, 'learning_rate': 5.201342281879195e-07, 'epoch': 0.98}\n",
      "{'loss': 0.7869, 'learning_rate': 4.362416107382551e-07, 'epoch': 0.98}\n",
      "{'loss': 0.754, 'learning_rate': 3.523489932885906e-07, 'epoch': 0.99}\n",
      "{'loss': 0.7215, 'learning_rate': 2.684563758389262e-07, 'epoch': 0.99}\n",
      "{'loss': 0.8705, 'learning_rate': 1.8456375838926178e-07, 'epoch': 0.99}\n",
      "{'loss': 0.6593, 'learning_rate': 1.0067114093959732e-07, 'epoch': 1.0}\n",
      "{'train_runtime': 5747.3418, 'train_samples_per_second': 1.729, 'train_steps_per_second': 0.432, 'train_loss': 0.8732805134977504, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2484, training_loss=0.8732805134977504, metrics={'train_runtime': 5747.3418, 'train_samples_per_second': 1.729, 'train_steps_per_second': 0.432, 'train_loss': 0.8732805134977504, 'epoch': 1.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AdamW\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "output_dir = './results'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "print(f\"Output directory is set to: {output_dir}\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    for label_name, label_id in {'True': 0, 'False': 1, 'Conflicting': 2}.items():\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(labels, preds, labels=[label_id], average='weighted')\n",
    "        metrics[f'{label_name}_precision'] = precision[0]\n",
    "        metrics[f'{label_name}_recall'] = recall[0]\n",
    "        metrics[f'{label_name}_f1'] = f1[0]\n",
    "        metrics[f'{label_name}_support'] = support[0]\n",
    "\n",
    "    #overall accuracy\n",
    "    metrics['accuracy'] = accuracy_score(labels, preds)\n",
    "    return metrics\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps = 5000,\n",
    "    use_cpu=False,\n",
    "    dataloader_pin_memory=False\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(optimizer, None),\n",
    "    compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1be7f6035b044c58d80433928988edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9082304239273071, 'eval_accuracy': 0.5627254509018036, 'eval_precision': 0.5666230665846471, 'eval_recall': 0.5627254509018036, 'eval_f1': 0.5640043334371488, 'eval_support': None, 'eval_runtime': 73.8159, 'eval_samples_per_second': 33.8, 'eval_steps_per_second': 8.453, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate(test_dataset)\n",
    "\n",
    "results_file = os.path.join(training_args.output_dir, \"evaluation_results.txt\")\n",
    "with open(results_file, \"w\") as writer:\n",
    "    for key, value in results.items():\n",
    "        writer.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
