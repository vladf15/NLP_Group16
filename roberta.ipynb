{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "# Use cuda if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\", num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#inspired by the oracle code\n",
    "#https://colab.research.google.com/drive/1gZJCakmY28cKGMj8B7wd1GUM3r72pdbi?usp=sharing#scrollTo=8eKFjiC3i8Yx\n",
    "# Function to get labels and claims from the dataset\n",
    "\n",
    "train_df = pd.read_json('train_claims_quantemp.json')\n",
    "val_df = pd.read_json('val_claims_quantemp.json')\n",
    "#test_df = pd.read_json('test_claims_quantemp.json')\n",
    "\n",
    "train_df = pd.DataFrame([{'label': item['label'], 'claim': item['claim']} for item in train_df.to_dict(orient='records')])\n",
    "val_df = pd.DataFrame([{'label': item['label'], 'claim': item['claim']} for item in val_df.to_dict(orient='records')])\n",
    "#test_df = pd.DataFrame([{'label': item['label'], 'claim': item['claim']} for item in test_df.to_dict(orient='records')])\n",
    "\n",
    "#old evidence file for bm25\n",
    "#evidence_df = pd.read_json('bm25_top_100_claimdecomp.json')\n",
    "#evidence_df = pd.DataFrame([{'claim': item['claim'], 'docs': item['docs'], 'scores': item['scores']} for item in evidence_df.to_dict(orient='records')])\n",
    "\n",
    "#load csv evidence files for training and validation\n",
    "train_evidence_df = pd.read_csv('NLP_Group16/evidences_train.csv')\n",
    "val_evidence_df = pd.read_csv('NLP_Group16/evidences_val.csv')\n",
    "train_evidence_df = train_evidence_df[['claim', 'evidences', 'scores']]\n",
    "val_evidence_df = val_evidence_df[['claim', 'evidences', 'scores']]\n",
    "\n",
    "\n",
    "# Get top_k relevant evidence for each claim\n",
    "def combine_top_k_evidence(row, top_k):\n",
    "    evidences = row['evidences'].strip('[]').split(', ')\n",
    "    scores = list(map(float, row['scores'].strip('[]').split(', ')))\n",
    "    ranked_documents = sorted(zip(evidences, scores), key=lambda x: x[1], reverse=True)\n",
    "    top_k_documents = [doc.strip('\"') for doc, score in ranked_documents[:top_k]]\n",
    "    return ' '.join(top_k_documents)\n",
    "\n",
    "top_k = 5\n",
    "# Apply the function to get top_k evidence for each claim\n",
    "train_evidence_df['top_k_docs'] = train_evidence_df.apply(lambda row: combine_top_k_evidence(row, top_k), axis=1)\n",
    "val_evidence_df['top_k_docs'] = val_evidence_df.apply(lambda row: combine_top_k_evidence(row, top_k), axis=1)\n",
    "\n",
    "# Drop docs and scores columns as they are no longer needed\n",
    "train_evidence_df = train_evidence_df.drop(columns=['evidences', 'scores'])\n",
    "val_evidence_df = val_evidence_df.drop(columns=['evidences', 'scores'])\n",
    "\n",
    "# Merge the combined evidence with the train, validation, and test DataFrames\n",
    "train_df = train_df.merge(train_evidence_df[['claim', 'top_k_docs']], on='claim', how='left')\n",
    "val_df = val_df.merge(val_evidence_df[['claim', 'top_k_docs']], on='claim', how='left')\n",
    "#test_df = test_df.merge(evidence_df[['claim', 'top_k_docs']], on='claim', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vladf.DESKTOP-LI3R2S2\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def tokenize_data(df, max_length = 256):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        df['claim'].tolist(),\n",
    "        df['top_k_docs'].fillna('').tolist(),  # Fill NaN with empty strings\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = df['label'].apply(lambda x: {'False': 0, 'True': 1, 'Conflicting': 2}[x]).tolist()\n",
    "    return tokenized_inputs, labels\n",
    "\n",
    "train_tokenized, train_labels = tokenize_data(train_df)\n",
    "val_tokenized, val_labels = tokenize_data(val_df)\n",
    "#test_tokenized, test_labels = tokenize_data(test_df)\n",
    "\n",
    "print(train_tokenized['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ClaimsDataset(Dataset):\n",
    "    def __init__(self, tokenized_inputs, labels):\n",
    "        self.input_ids = tokenized_inputs['input_ids']\n",
    "        self.attention_mask = tokenized_inputs['attention_mask']\n",
    "        self.labels = torch.tensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = ClaimsDataset(train_tokenized, train_labels)\n",
    "val_dataset = ClaimsDataset(val_tokenized, val_labels)\n",
    "#test_dataset = ClaimsDataset(test_tokenized, test_labels)\n",
    "\n",
    "train_tokenized = {key: val.to(device) for key, val in train_tokenized.items()}\n",
    "val_tokenized = {key: val.to(device) for key, val in val_tokenized.items()}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers= 4, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers= 4, pin_memory=True)\n",
    "#test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=torch.inf)\n",
    "print(train_evidence_df[:3])\n",
    "\n",
    "for i, batch in enumerate(train_dataset):\n",
    "    assert batch['input_ids'].shape == batch['attention_mask'].shape\n",
    "    print(f\"Batch {i+1}\")\n",
    "    print(batch)\n",
    "    if i == 2:  # Only print the first 3 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 260:\n",
      "Predictions: [0]\n",
      "Labels: [2]\n",
      "{'loss': 0.0096, 'learning_rate': 1.9685183304559766e-05, 'epoch': 0.03}\n",
      "Step 261:\n",
      "Predictions: [0]\n",
      "Labels: [2]\n",
      "{'loss': 0.0084, 'learning_rate': 1.9683152229105313e-05, 'epoch': 0.03}\n",
      "Step 262:\n",
      "Predictions: [0]\n",
      "Labels: [0]\n",
      "{'loss': 5.3125, 'learning_rate': 1.968112115365086e-05, 'epoch': 0.03}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback, TrainerState, TrainerControl, AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "output_dir = './results'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "print(f\"Output directory is set to: {output_dir}\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    metrics = {}\n",
    "\n",
    "    # Compute macro F1-score\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    metrics['f1_macro'] = f1_macro\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    metrics['accuracy'] = accuracy_score(labels, preds)\n",
    "    \n",
    "    # Compute metrics for each label\n",
    "    for label_name, label_id in {'False': 0, 'True': 1, 'Conflicting': 2}.items():\n",
    "        precision, recall, f1_weighted, support = precision_recall_fscore_support(labels, preds, labels=[label_id], average='weighted')\n",
    "        metrics[f'{label_name}_precision'] = precision\n",
    "        metrics[f'{label_name}_recall'] = recall\n",
    "        metrics[f'{label_name}_f1_weighted'] = f1_weighted\n",
    "        metrics[f'{label_name}_support'] = support\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1,\n",
    "    save_steps = 5000,\n",
    "    # Specify to use CUDA\n",
    "    use_cpu=False,\n",
    "    fp16=True,  # Enable mixed precision training\n",
    ")\n",
    "\n",
    "\n",
    "label_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=[0, 1, 2],\n",
    "    y=train_df['label'].apply(lambda x: {'False': 0, 'True': 1, 'Conflicting': 2}[x])\n",
    ")\n",
    "label_weights = torch.tensor(label_weights, dtype=torch.float).to(device)\n",
    "print(f\"Label weights: {label_weights}\")\n",
    "\n",
    "\n",
    "# logging predictions and labels to check if the model is learning\n",
    "class PredictionLoggerCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % args.logging_steps == 0:\n",
    "            model = kwargs.get('model')\n",
    "            dataloader = kwargs.get('train_dataloader')\n",
    "            \n",
    "            if model and dataloader:\n",
    "                model.eval()\n",
    "                batch = next(iter(dataloader))\n",
    "                inputs = {key: value.to(args.device) for key, value in batch.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    preds = torch.argmax(logits, axis=-1)\n",
    "                    labels = inputs['labels']\n",
    "                    \n",
    "                    print(f\"Step {state.global_step}:\")\n",
    "                    print(f\"Predictions: {preds.cpu().numpy()}\")\n",
    "                    print(f\"Labels: {labels.cpu().numpy()}\")\n",
    "                model.train()\n",
    "\n",
    "        return control\n",
    "\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=label_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "    \n",
    "log_callback = PredictionLoggerCallback()\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(optimizer, None),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[log_callback]\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(val_dataset)\n",
    "\n",
    "results_file = os.path.join(training_args.output_dir, \"evaluation_results.txt\")\n",
    "with open(results_file, \"w\") as writer:\n",
    "    for key, value in results.items():\n",
    "        writer.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Run predictions on the test set to perform qualitative analysis\n",
    "predictions_output = trainer.predict(val_dataset)\n",
    "predictions = np.argmax(predictions_output.predictions, axis=1)\n",
    "labels = predictions_output.label_ids\n",
    "\n",
    "# Identify failed predictions\n",
    "failed_indices = np.where(predictions != labels)[0]\n",
    "\n",
    "# Extract the corresponding claims and predictions\n",
    "failed_claims = val_df.iloc[failed_indices]\n",
    "failed_predictions = predictions[failed_indices]\n",
    "failed_labels = labels[failed_indices]\n",
    "\n",
    "# Randomly select claims for qualitative analysis\n",
    "num_samples = 20\n",
    "random_indices = random.sample(range(len(failed_indices)), min(num_samples, len(failed_indices)))\n",
    "\n",
    "# Extract the randomly selected failed claims, predictions, and true labels\n",
    "selected_failed_claims = failed_claims.iloc[random_indices]\n",
    "selected_failed_predictions = failed_predictions[random_indices]\n",
    "selected_failed_labels = failed_labels[random_indices]\n",
    "\n",
    "# Create a DataFrame for qualitative analysis\n",
    "qualitative_analysis_df = pd.DataFrame({\n",
    "    'claim': selected_failed_claims['claim'].values,\n",
    "    'top_k_docs': selected_failed_claims['top_k_docs'].values,\n",
    "    'real_label': selected_failed_labels,\n",
    "    'predicted_label': selected_failed_predictions\n",
    "})\n",
    "\n",
    "# Map numerical labels to their string representations\n",
    "label_map = {0: 'False', 1: 'True', 2: 'Conflicting'}\n",
    "qualitative_analysis_df['real_label'] = qualitative_analysis_df['real_label'].map(label_map)\n",
    "qualitative_analysis_df['predicted_label'] = qualitative_analysis_df['predicted_label'].map(label_map)\n",
    "\n",
    "# Save the qualitative analysis results to a text file\n",
    "qualitative_analysis_file = os.path.join(training_args.output_dir, \"qualitative_analysis.txt\")\n",
    "with open(qualitative_analysis_file, \"w\") as writer:\n",
    "    for index, row in qualitative_analysis_df.iterrows():\n",
    "        writer.write(f\"Claim: {row['claim']}\\n\")\n",
    "        writer.write(f\"Evidence: {row['top_k_docs']}\\n\")\n",
    "        writer.write(f\"Real Label: {row['real_label']}\\n\")\n",
    "        writer.write(f\"Predicted Label: {row['predicted_label']}\\n\")\n",
    "        writer.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_counts = pd.Series(train_labels).value_counts()\n",
    "print(train_label_counts)\n",
    "val_label_counts = pd.Series(val_labels).value_counts()\n",
    "print(val_label_counts)\n",
    "prediction_label_counts = pd.Series(predictions).value_counts()\n",
    "print(prediction_label_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
