# -*- coding: utf-8 -*-
"""OpenAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TTpfMggWchiVD4adgdVDYySI8scCozod
"""

!pip install datasets OpenAI

from openai import OpenAI
client = OpenAI(api_key="", project="")

from google.colab import drive
drive.mount('/content/drive')

from datasets import load_dataset
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
import pandas as pd

BASE_PATH = "/content/drive/My Drive/"
DATASET_BASE_PATH = BASE_PATH + "StrategyQA/"
DATASET_TRAIN_PATH = DATASET_BASE_PATH + "strategyqa_train.json"

df_all = pd.read_json(DATASET_TRAIN_PATH)
df_all = df_all.drop(["qid","term", "description", "evidence", "answer"], axis=1)
ds_all = Dataset.from_pandas(df_all)
# train,val = train_test_split(df_all,test_size=0.10,random_state=42)
# raw_datasets = DatasetDict({
#     "train": Dataset.from_pandas(train),
#     "val": Dataset.from_pandas(val)
# })

pred2 = []

for question in ds_all['question']:
  completion = client.chat.completions.create(
    model="gpt-4o",
    messages=[
      {"role": "system", "content": "You will be provided with a question, and your task is to label this question with these types: interval, comparison, statistical or temporal. One question can have mutliple labels. Only report the labels seperated with comma's."},
      {"role": "user", "content": question}
    ]
    )
  pred2.append(completion.choices[0].message.content)
print(pred2)

print(pred2[:5])
print(ds_all['question'][:5])

print(raw_datasets["train"]["question"][5])

p = [p.lower() for p in pred]

print('temporal')

ds = raw_datasets["train"].add_column("question_type", p)

# def to_number(label):
#   match label:
#     case "TRUE":
#       return 0
#     case "FALSE":
#       return 1
#     case "CONFLICTING":
#       return 2
#     case _:
#       return 3

# prediction_nums = [to_number(p) for p in predictions]

# print(len(predictions))

# predictions_100_1000 = predictions[47:]
# prediction_nums_100_1000 = [to_number(p) for p in predictions_100_1000]
# print(len(prediction_nums_100_1000))

# print(sum([1 if y_pred[i] == y_true[i] else 0 for i in range(0,900)]))

# print(y_pred)

ds['question'][1]
ds['question_type'][1]

import numpy as np
from sklearn.metrics import f1_score
y_true = [to_number(l) for l in val_dataset['label'][1000:1250]]
y_pred = [to_number(l) for l in pred_1000_1250]

print(y_pred)

print(f1_score(y_true, y_pred, average='macro'))
print(f1_score(y_true, y_pred, average='weighted'))

# ds.save_to_disk("/content/drive/My Drive/strategyqa-train-with-types.hf")
ds.to_json("/content/drive/My Drive/strategyqa-train-with-types.jsonl")

completion.choices[0].message

!pip install datasets

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from datasets import Dataset
from ast import literal_eval

BASE_LOCATION = "/content/drive/My Drive/"
TRAIN_LOCATION = BASE_LOCATION + "train_top_20.csv"
VAL_LOCATION = BASE_LOCATION + "val_top_20.csv"

train_dataset = pd.read_csv(TRAIN_LOCATION, index_col=0)
train_dataset['evidences'] = train_dataset['evidences'].apply(literal_eval)
train_dataset['scores'] = train_dataset['scores'].apply(literal_eval)
train_dataset = Dataset.from_pandas(train_dataset)

val_dataset = pd.read_csv(VAL_LOCATION, index_col=0)
val_dataset['evidences'] = val_dataset['evidences'].apply(literal_eval)
val_dataset['scores'] = val_dataset['scores'].apply(literal_eval)
val_dataset = Dataset.from_pandas(val_dataset)

def generate_prompt(examples):
  prompts = []
  for i in range(len(examples['claim'])):
    claim = examples['claim'][i]
    evidences = list(zip(examples['evidences'][i], examples['scores'][i]))

    prompt_begin = (
        f"Claim: {claim}\n\n"
        f"Evidences:\n"
    )
    prompt_evidences = f""
    prompt_end = f"\n\nLabel (TRUE, FALSE, CONFLICTING):"

    for k in range(0, min(5, len(evidences))):
        new_evidence = f"\n{k + 1}. {evidences[k][0]} (score: {round(evidences[k][1], 2)})"
        prompt_evidences += new_evidence
    prompts.append(prompt_begin + prompt_evidences + prompt_end)
  return {"prompts": prompts}

dataset = val_dataset.map(generate_prompt, batched=True, remove_columns=val_dataset.column_names)

print(dataset["prompts"][3])

from collections import Counter

# print(Counter(pred2))

# pred2_clean = [item.strip() for sublist in pred2 for item in sublist.split(',')]

valid_labels  = ["comparison", "statistical", "temporal", "interval"]

pred2_clean = [a.split(',') for a in pred2]
for i in range(len(pred2_clean)):
  print()
  pred2_clean[i] = set([a.strip() for a in pred2_clean[i]])
  pred2_clean[i] = tuple(pred2_clean[i].intersection(valid_labels))
  # for j in range(len(pred2_clean[j])):

# pred2_clean = [print(a) for a in pred2_clean[:3]]
# print(pred2_clean[:3])

print(Counter(pred2_clean))

# pred2[2]
print(pred2_clean[0:5])
print(ds_all['question'][:5])

pred2_clean

ds2 = ds_all.add_column("question_type", pred2_clean)

ds2['question_type'][0]

ds2.save_to_disk("/content/drive/My Drive/strategyqa-train-with-multiple-types.hf")
ds2.to_json("/content/drive/My Drive/strategyqa-train-with-multiple-types.jsonl")

temporals = ds2.filter(lambda example: 'temporal' in example["question_type"] or 'interval' in example["question_type"])

temporals