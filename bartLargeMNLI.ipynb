{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbfdcf3-8967-4d22-bf1d-65afd839d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import json\n",
    "import sentencepiece\n",
    "import rouge\n",
    "import numpy as np\n",
    "import transformers\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import BertTokenizer,AutoModelForSeq2SeqLM, AutoModelForMaskedLM, EvalPrediction, BertForSequenceClassification, Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer, BartTokenizer, BartForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b0d339-4599-49b5-9dcd-6fdbc6683753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a79a8a-1380-44a7-8167-bb783800c692",
   "metadata": {},
   "source": [
    "# Data loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca844d43-1cdb-4874-b6eb-fc00bfce4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your datasets\n",
    "train_path = 'NumTemp-E9C0/data/raw_data/train_claims_quantemp.json'\n",
    "val_path = 'NumTemp-E9C0/data/raw_data/val_claims_quantemp.json'\n",
    "test_path = 'NumTemp-E9C0/data/raw_data/test_claims_quantemp.json'\n",
    "evidence_path = 'NumTemp-E9C0/data/corpus_evidence_unified.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4eb4f1-60b8-4038-976a-b3cfef7be09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to extract and create dataframes\n",
    "\n",
    "def extract_claims_and_labels(file_path):\n",
    "    \"\"\"\n",
    "    Extracts claims and labels from the given JSON file and converts them into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the claims and labels.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extract only the \"label\" and \"claim\" fields\n",
    "    extracted_data = [{'label': item['label'], 'claim': item['claim']} for item in data]\n",
    "\n",
    "    # Convert the extracted data to a DataFrame\n",
    "    df_claims = pd.DataFrame(extracted_data)\n",
    "\n",
    "    return df_claims\n",
    "\n",
    "# Convert the extracted data to a DataFrame\n",
    "df_claim_train = extract_claims_and_labels(train_path)\n",
    "df_claim_val = extract_claims_and_labels(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e8904-4f5b-496e-a143-e412c8675e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract evidence snippits from the corpus\n",
    "with open(evidence_path, 'r', encoding='utf-8') as file:\n",
    "    evidence_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0adffe9-665e-468a-abca-bd110492e863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Evidence Collector\n",
    "# Initialize BERT tokenizer and model\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Move model to the GPU\n",
    "bert_model.to(device)\n",
    "\n",
    "corpus = list(evidence_data.values())\n",
    "\n",
    "# Tokenize the entire corpus once\n",
    "def tokenize_corpus(corpus):\n",
    "    return [doc.split(\" \") for doc in corpus]\n",
    "\n",
    "# Function to retrieve top-k documents using BM25 and re-rank using BERT\n",
    "def retrieve_evidence(query, bm25, corpus, k=5):\n",
    "    # BM25 retrieval\n",
    "    tokenized_query = query.split(\" \")\n",
    "    bm25_top_k = bm25.get_top_n(tokenized_query, corpus, n=250)  # Adjust 'n' as needed\n",
    "    \n",
    "    # Tokenize the query once for BERT\n",
    "    tokenized_query_bert = bert_tokenizer.encode_plus(query, add_special_tokens=True, return_tensors='pt', truncation=True, max_length=512)\n",
    "    tokenized_query_bert = {key: value.to(device) for key, value in tokenized_query_bert.items()}\n",
    "    \n",
    "    # Tokenize the BM25 top-k documents for BERT\n",
    "    tokenized_corpus_bert = [bert_tokenizer.encode_plus(doc, add_special_tokens=True, return_tensors='pt', truncation=True, max_length=512) for doc in bm25_top_k]\n",
    "    \n",
    "    # Process documents in batches\n",
    "    batch_size = 64\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(0, len(tokenized_corpus_bert), batch_size):\n",
    "        batch = tokenized_corpus_bert[i:i+batch_size]\n",
    "        batch = [{key: value.to(device) for key, value in doc.items()} for doc in batch]  # Move each document to GPU\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = [bert_model(**{**tokenized_query_bert, **doc}) for doc in batch]  # Perform inference on GPU\n",
    "            batch_scores = [output.logits.squeeze().item() for output in outputs]\n",
    "        \n",
    "        scores.extend(batch_scores)\n",
    "    \n",
    "    # Re-rank documents based on scores\n",
    "    ranked_documents = sorted(zip(bm25_top_k, scores), key=lambda x: x[1], reverse=True)\n",
    "    top_k_evidences = [doc for doc, score in ranked_documents[:k]]\n",
    "    \n",
    "    return top_k_evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a6ead-4907-4d61-8639-047da7f7a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_top_k_evidences(df, bm25, corpus, k=5):\n",
    "    df['top_k_evidences'] = df['claim'].progress_apply(lambda claim: retrieve_evidence(claim, bm25, corpus, k))\n",
    "    return df\n",
    "\n",
    "# Assuming df_claim_train and df_claim_val are your DataFrames containing the claims and labels\n",
    "# Tokenize the corpus once for BM25\n",
    "#tokenized_corpus = tokenize_corpus(corpus)\n",
    "#bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "#df_claim_train = add_top_k_evidences(df_claim_train, bm25, corpus, k=5)\n",
    "#df_claim_val = add_top_k_evidences(df_claim_val, bm25, corpus, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a90571-1527-4c7e-ad3c-79ad5cdc1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the DataFrames from CSV files\n",
    "#df_claim_train.to_csv('df_claim_train.csv', index=False)\n",
    "#df_claim_val.to_csv('df_claim_val.csv', index=False)\n",
    "\n",
    "# Load the DataFrames from CSV files\n",
    "#df_claim_train = pd.read_csv('df_claim_train.csv')\n",
    "#df_claim_val = pd.read_csv('df_claim_val.csv')\n",
    "\n",
    "df_claim_train = pd.read_csv('evidences_train.csv')\n",
    "df_claim_val = pd.read_csv('evidences_val.csv')\n",
    "df_claim_test = pd.read_csv('evidences_test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff97e4a-3fd5-4392-8602-d1383b347a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df_claim_train['label'] = label_encoder.fit_transform(df_claim_train['label'])\n",
    "df_claim_val['label'] = label_encoder.fit_transform(df_claim_val['label'])\n",
    "df_claim_test['label'] = label_encoder.fit_transform(df_claim_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d4549-a080-46a1-9487-72e903417132",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claim_val.drop(columns=['Unnamed: 0', 'scores'], inplace=True)\n",
    "df_claim_train.drop(columns=['Unnamed: 0', 'scores'], inplace=True)\n",
    "#df_claim_test.drop(columns=['Unnamed: 0', 'scores'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9369fd-146f-4f8e-a87c-89ffe283e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error()\n",
    "final_dataset_train = Dataset.from_pandas(df_claim_train)\n",
    "final_dataset_val = Dataset.from_pandas(df_claim_val)\n",
    "final_dataset_test = Dataset.from_pandas(df_claim_test)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['evidences'], examples['claim'], truncation=True, padding='max_length', max_length=1024)\n",
    "\n",
    "#encoded_dataset_train = final_dataset_train.map(preprocess_function, batched=True)\n",
    "#encoded_dataset_val = final_dataset_val.map(preprocess_function, batched=True)\n",
    "encoded_dataset_test = final_dataset_test.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb0ea6-8711-42dd-807c-982116bb808f",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2f987-4b65-40de-be34-35e8f745a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset_test['input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a674b8-0234-4a03-970e-2671867e2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose and load tokenizer and model\n",
    "\n",
    "#BART\n",
    "model_name = \"facebook/bart-large-mnli\" \n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "#MathRoberta\n",
    "#model_name = \"nielsr/nt5-small-rc1\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model =  AutoModelForSeq2SeqLM.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e99a0-a76f-44cf-8639-eaf8ba8bea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "transformers.logging.set_verbosity(logging.WARNING)\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "\n",
    "    print(f'Predictions shape: {preds}')\n",
    "    print(f'Predictions shape: {p.predictions}')\n",
    "    print(f'Labels shape: {p.label_ids}')\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak. \n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels\n",
    "    \n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    f1_weighted_average = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    #roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1 micro': f1_micro_average,\n",
    "               'f1 macro': f1_macro_average,\n",
    "               'f1 weighted': f1_weighted_average,\n",
    "               #'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482889d6-de0f-4062-b195-c2fd59eb6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checkpoint\n",
    "#checkpoint = './results/checkpoint-2000'\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps = 1000,\n",
    "    # Specify to use CUDA\n",
    "    use_cpu=False,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset_train,\n",
    "    eval_dataset=encoded_dataset_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "#trainer.train()\n",
    "\n",
    "#if from checkpoint\n",
    "#trainer.train(resume_from_checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61719f9-b796-40ac-bb52-9a0e5959c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save or Load a model\n",
    "#model.save_pretrained('./finetuned_math')\n",
    "#tokenizer.save_pretrained('./finetuned_bart')\n",
    "\n",
    "# Load the fine-tuned BART model\n",
    "model = BartForSequenceClassification.from_pretrained('./models/finetuned_BART', device=0)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('./models/finetuned_BART', device=0)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a38df6-25b3-44d8-9fa7-bbde68e8cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate a model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f6a31-1ab4-4e42-a4c7-afc3eee63666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "context = \"\"\"Saint Jean de Brébeuf was a French Jesuit missionary who\n",
    "travelled to New France in 1625. There he worked primarily with the Huron\n",
    "for the rest of his life, except for a few years in France from 1629 to\n",
    "1633. He learned their language and culture, writing extensively about\n",
    "each to aid other missionaries. In 1649, Br´ebeuf and another missionary\n",
    "were captured when an Iroquois raid took over a Huron village . Together\n",
    "with Huron captives, the missionaries were ritually tortured and killed\n",
    "on March 16, 1649. Br´ebeuf was beatified in 1925 and among eight Jesuit\n",
    "missionaries canonized as saints in the Roman Catholic Church in 1930.\"\"\"\n",
    "\n",
    "question = \"How many years did Saint Jean de Brébeuf stay in New France before he went back to France for a few years?\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"nielsr/nt5-small-rc1\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"nielsr/nt5-small-rc1\")\n",
    "\n",
    "# encode context & question\n",
    "input_text = f\"answer_me: {question} context: {context}\"\n",
    "encoded_query = tokenizer(\n",
    "                    input_text, \n",
    "                    return_tensors='pt', \n",
    "                    padding='max_length', \n",
    "                    truncation=True, \n",
    "                    max_length=512)\n",
    "\n",
    "# generate answer\n",
    "generated_answer = model.generate(input_ids=encoded_query[\"input_ids\"], \n",
    "                                  attention_mask=encoded_query[\"attention_mask\"], \n",
    "                                  max_length=54)\n",
    "\n",
    "decoded_answer = tokenizer.decode(generated_answer.numpy()[0])\n",
    "print(\"T5 Answer: \", decoded_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd3043-a256-4dc2-8163-c4b9ad9c2130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
